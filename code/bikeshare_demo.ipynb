{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "readable_labels = {\n",
    "     0  : \"day\",\n",
    "     1  : \"season\",\n",
    "     2  : \"year\",\n",
    "     3  : \"month\",\n",
    "     4  : \"hour\",\n",
    "\n",
    "     5  : \"holiday\",\n",
    "     6  : \"day of week\",\n",
    "     7  : \"workday\",\n",
    "\n",
    "     8  : \"weather\",\n",
    "     9  : \"temperature\",\n",
    "     10 : \"feels_like_temp\",\n",
    "     11 : \"humidity\",\n",
    "     12 : \"wind speed\",\n",
    "}\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO DOWNLOAD BIKE SHARING DATASET FROM\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n",
    "\n",
    "AND EXTRACT \"hour.csv\" INTO THE \"data/\" FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- processed and saved ---\n"
     ]
    }
   ],
   "source": [
    "from data_loader import preprocess_bike_sharing_dataset\n",
    "preprocess_bike_sharing_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "mean training loss\t 0.897428768189227\n",
      "epoch 0 '\n",
      "MSE for train and val\t 0.7365915393118533 \t 0.7235485812341984\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 1\n",
      "mean training loss\t 0.6566467764924784\n",
      "epoch 1 '\n",
      "MSE for train and val\t 0.5696789414474533 \t 0.5770612433472739\n",
      "--- 0.769 seconds in epoch ---\n",
      "\n",
      "Epoch 2\n",
      "mean training loss\t 0.5386885991839112\n",
      "epoch 2 '\n",
      "MSE for train and val\t 0.44142414515113737 \t 0.4460589352257736\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 3\n",
      "mean training loss\t 0.45154831971301407\n",
      "epoch 3 '\n",
      "MSE for train and val\t 0.31981868112414885 \t 0.3329066028809113\n",
      "--- 0.779 seconds in epoch ---\n",
      "\n",
      "Epoch 4\n",
      "mean training loss\t 0.3912725919094242\n",
      "epoch 4 '\n",
      "MSE for train and val\t 0.39339553807375865 \t 0.41255914089225937\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 5\n",
      "mean training loss\t 0.3476081473416969\n",
      "epoch 5 '\n",
      "MSE for train and val\t 0.2517777222402544 \t 0.27014757138281303\n",
      "--- 0.775 seconds in epoch ---\n",
      "\n",
      "Epoch 6\n",
      "mean training loss\t 0.31976566910743714\n",
      "epoch 6 '\n",
      "MSE for train and val\t 0.21773987184619412 \t 0.23943266060171187\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 7\n",
      "mean training loss\t 0.2997664397857228\n",
      "epoch 7 '\n",
      "MSE for train and val\t 0.21268103552225193 \t 0.23496881734688987\n",
      "--- 0.804 seconds in epoch ---\n",
      "\n",
      "Epoch 8\n",
      "mean training loss\t 0.28485328804274074\n",
      "epoch 8 '\n",
      "MSE for train and val\t 0.197792269288034 \t 0.22287886802197304\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 9\n",
      "mean training loss\t 0.27234950148668446\n",
      "epoch 9 '\n",
      "MSE for train and val\t 0.17988024815634338 \t 0.2050047686759335\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 10\n",
      "mean training loss\t 0.26195676497748643\n",
      "epoch 10 '\n",
      "MSE for train and val\t 0.16438962798711976 \t 0.189927183367942\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 11\n",
      "mean training loss\t 0.2532351854394694\n",
      "epoch 11 '\n",
      "MSE for train and val\t 0.16809097014114824 \t 0.19221407608484556\n",
      "--- 0.802 seconds in epoch ---\n",
      "\n",
      "Epoch 12\n",
      "mean training loss\t 0.2451857847268464\n",
      "epoch 12 '\n",
      "MSE for train and val\t 0.14712910295361092 \t 0.17225028925309827\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 13\n",
      "mean training loss\t 0.23885759965318148\n",
      "epoch 13 '\n",
      "MSE for train and val\t 0.16803004226284973 \t 0.19779521932015354\n",
      "--- 0.806 seconds in epoch ---\n",
      "\n",
      "Epoch 14\n",
      "mean training loss\t 0.2333987808618389\n",
      "epoch 14 '\n",
      "MSE for train and val\t 0.14200328346140023 \t 0.16863147251120214\n",
      "--- 0.787 seconds in epoch ---\n",
      "\n",
      "Epoch 15\n",
      "mean training loss\t 0.22805594186313816\n",
      "epoch 15 '\n",
      "MSE for train and val\t 0.1362507199883338 \t 0.1630441771294551\n",
      "--- 0.779 seconds in epoch ---\n",
      "\n",
      "Epoch 16\n",
      "mean training loss\t 0.22340040099425393\n",
      "epoch 16 '\n",
      "MSE for train and val\t 0.12951910654442172 \t 0.15577816220376775\n",
      "--- 0.756 seconds in epoch ---\n",
      "\n",
      "Epoch 17\n",
      "mean training loss\t 0.21943839431786147\n",
      "epoch 17 '\n",
      "MSE for train and val\t 0.14987231689699385 \t 0.17659627962639654\n",
      "--- 0.786 seconds in epoch ---\n",
      "\n",
      "Epoch 18\n",
      "mean training loss\t 0.21505547488322024\n",
      "epoch 18 '\n",
      "MSE for train and val\t 0.12485548224195638 \t 0.155386893152144\n",
      "--- 0.768 seconds in epoch ---\n",
      "\n",
      "Epoch 19\n",
      "mean training loss\t 0.21175704275975463\n",
      "epoch 19 '\n",
      "MSE for train and val\t 0.12219355694014714 \t 0.15260829211930202\n",
      "--- 0.781 seconds in epoch ---\n",
      "\n",
      "Epoch 20\n",
      "mean training loss\t 0.20884237206373057\n",
      "epoch 20 '\n",
      "MSE for train and val\t 0.12075703042564401 \t 0.1494698512797266\n",
      "--- 0.763 seconds in epoch ---\n",
      "\n",
      "Epoch 21\n",
      "mean training loss\t 0.2059298638437615\n",
      "epoch 21 '\n",
      "MSE for train and val\t 0.11859278784393587 \t 0.1500755823602653\n",
      "--- 0.768 seconds in epoch ---\n",
      "\n",
      "Epoch 22\n",
      "mean training loss\t 0.2033172406134058\n",
      "epoch 22 '\n",
      "MSE for train and val\t 0.11062066431826344 \t 0.14148540237942514\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 23\n",
      "mean training loss\t 0.2007625681943581\n",
      "epoch 23 '\n",
      "MSE for train and val\t 0.10888392799937946 \t 0.14045552403906206\n",
      "--- 0.758 seconds in epoch ---\n",
      "\n",
      "Epoch 24\n",
      "mean training loss\t 0.20026464696790353\n",
      "epoch 24 '\n",
      "MSE for train and val\t 0.12596508286958646 \t 0.15659489165941434\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 25\n",
      "mean training loss\t 0.19689810637567864\n",
      "epoch 25 '\n",
      "MSE for train and val\t 0.11032523204406179 \t 0.14304996806406847\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 26\n",
      "mean training loss\t 0.19455560729151866\n",
      "epoch 26 '\n",
      "MSE for train and val\t 0.10309713119584528 \t 0.13625091404000467\n",
      "--- 0.76 seconds in epoch ---\n",
      "\n",
      "Epoch 27\n",
      "mean training loss\t 0.1933966711407802\n",
      "epoch 27 '\n",
      "MSE for train and val\t 0.10332732853166521 \t 0.13511000077300167\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 28\n",
      "mean training loss\t 0.19151111061944337\n",
      "epoch 28 '\n",
      "MSE for train and val\t 0.09937795089663826 \t 0.1315394843059091\n",
      "--- 0.755 seconds in epoch ---\n",
      "\n",
      "Epoch 29\n",
      "mean training loss\t 0.1898885285756627\n",
      "epoch 29 '\n",
      "MSE for train and val\t 0.10162401935941462 \t 0.13240767413253593\n",
      "--- 0.753 seconds in epoch ---\n",
      "\n",
      "Epoch 30\n",
      "mean training loss\t 0.18816647666399597\n",
      "epoch 30 '\n",
      "MSE for train and val\t 0.09592528379479402 \t 0.12879035724512672\n",
      "--- 0.747 seconds in epoch ---\n",
      "\n",
      "Epoch 31\n",
      "mean training loss\t 0.18780396158089402\n",
      "epoch 31 '\n",
      "MSE for train and val\t 0.1367126325586626 \t 0.16858408078590445\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 32\n",
      "mean training loss\t 0.18602754048636702\n",
      "epoch 32 '\n",
      "MSE for train and val\t 0.09373676244578795 \t 0.12687058852105335\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 33\n",
      "mean training loss\t 0.18453153801746056\n",
      "epoch 33 '\n",
      "MSE for train and val\t 0.09295406818016608 \t 0.12626058005775004\n",
      "--- 0.746 seconds in epoch ---\n",
      "\n",
      "Epoch 34\n",
      "mean training loss\t 0.1833153398310552\n",
      "epoch 34 '\n",
      "MSE for train and val\t 0.09718782853682252 \t 0.13125884208756294\n",
      "--- 0.74 seconds in epoch ---\n",
      "\n",
      "Epoch 35\n",
      "mean training loss\t 0.1822277147261823\n",
      "epoch 35 '\n",
      "MSE for train and val\t 0.09231794990102443 \t 0.1257811888780738\n",
      "--- 0.785 seconds in epoch ---\n",
      "\n",
      "Epoch 36\n",
      "mean training loss\t 0.18118567205354816\n",
      "epoch 36 '\n",
      "MSE for train and val\t 0.09455101025882262 \t 0.1291437526604063\n",
      "--- 0.77 seconds in epoch ---\n",
      "\n",
      "Epoch 37\n",
      "mean training loss\t 0.18015466527860674\n",
      "epoch 37 '\n",
      "MSE for train and val\t 0.09589855456239457 \t 0.12825481039881279\n",
      "--- 0.763 seconds in epoch ---\n",
      "\n",
      "Epoch 38\n",
      "mean training loss\t 0.17930746481555407\n",
      "epoch 38 '\n",
      "MSE for train and val\t 0.09157709623491232 \t 0.12433747729445846\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 39\n",
      "mean training loss\t 0.17858337843027272\n",
      "epoch 39 '\n",
      "MSE for train and val\t 0.09832721713804937 \t 0.13224864863525626\n",
      "--- 0.769 seconds in epoch ---\n",
      "\n",
      "Epoch 40\n",
      "mean training loss\t 0.17723370919950673\n",
      "epoch 40 '\n",
      "MSE for train and val\t 0.08896104711349873 \t 0.12442971797784506\n",
      "--- 0.793 seconds in epoch ---\n",
      "\n",
      "Epoch 41\n",
      "mean training loss\t 0.1760995126894263\n",
      "epoch 41 '\n",
      "MSE for train and val\t 0.08657085782670589 \t 0.12100221803564036\n",
      "--- 0.785 seconds in epoch ---\n",
      "\n",
      "Epoch 42\n",
      "mean training loss\t 0.17570243297053165\n",
      "epoch 42 '\n",
      "MSE for train and val\t 0.09019955763545891 \t 0.12384129367951563\n",
      "--- 0.802 seconds in epoch ---\n",
      "\n",
      "Epoch 43\n",
      "mean training loss\t 0.17455279919944824\n",
      "epoch 43 '\n",
      "MSE for train and val\t 0.08839024713361662 \t 0.1221269604338944\n",
      "--- 0.757 seconds in epoch ---\n",
      "\n",
      "Epoch 44\n",
      "mean training loss\t 0.17371359242767584\n",
      "epoch 44 '\n",
      "MSE for train and val\t 0.0841969136077269 \t 0.11914842406485737\n",
      "--- 0.726 seconds in epoch ---\n",
      "\n",
      "Epoch 45\n",
      "mean training loss\t 0.173185800870911\n",
      "epoch 45 '\n",
      "MSE for train and val\t 0.08471682756025226 \t 0.11961454741957632\n",
      "--- 0.786 seconds in epoch ---\n",
      "\n",
      "Epoch 46\n",
      "mean training loss\t 0.17212130810882226\n",
      "epoch 46 '\n",
      "MSE for train and val\t 0.08682120580473345 \t 0.12070773627712861\n",
      "--- 0.777 seconds in epoch ---\n",
      "\n",
      "Epoch 47\n",
      "mean training loss\t 0.17194086149579188\n",
      "epoch 47 '\n",
      "MSE for train and val\t 0.08358252356047201 \t 0.11948198941602775\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 48\n",
      "mean training loss\t 0.17063773411219238\n",
      "epoch 48 '\n",
      "MSE for train and val\t 0.08676581894842265 \t 0.12224300965706714\n",
      "--- 0.775 seconds in epoch ---\n",
      "\n",
      "Epoch 49\n",
      "mean training loss\t 0.17051377660426936\n",
      "epoch 49 '\n",
      "MSE for train and val\t 0.08223478082163328 \t 0.11672349657524175\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 50\n",
      "mean training loss\t 0.1695020255983853\n",
      "epoch 50 '\n",
      "MSE for train and val\t 0.08187911976176732 \t 0.11769360017852373\n",
      "--- 0.731 seconds in epoch ---\n",
      "\n",
      "Epoch 51\n",
      "mean training loss\t 0.1705280567290353\n",
      "epoch 51 '\n",
      "MSE for train and val\t 0.09889399951058765 \t 0.13508393802378116\n",
      "--- 0.754 seconds in epoch ---\n",
      "\n",
      "Epoch 52\n",
      "mean training loss\t 0.16815331617339713\n",
      "epoch 52 '\n",
      "MSE for train and val\t 0.08000147065890861 \t 0.11538669896614852\n",
      "--- 0.787 seconds in epoch ---\n",
      "\n",
      "Epoch 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.16812098903245612\n",
      "epoch 53 '\n",
      "MSE for train and val\t 0.0800139434861647 \t 0.11573793903160864\n",
      "--- 0.749 seconds in epoch ---\n",
      "\n",
      "Epoch 54\n",
      "mean training loss\t 0.16722080688007543\n",
      "epoch 54 '\n",
      "MSE for train and val\t 0.08004345854801767 \t 0.11597826895100924\n",
      "--- 0.779 seconds in epoch ---\n",
      "\n",
      "Epoch 55\n",
      "mean training loss\t 0.16625496180819682\n",
      "epoch 55 '\n",
      "MSE for train and val\t 0.07846537460214709 \t 0.11465914339011635\n",
      "--- 0.805 seconds in epoch ---\n",
      "\n",
      "Epoch 56\n",
      "mean training loss\t 0.16772522816403967\n",
      "epoch 56 '\n",
      "MSE for train and val\t 0.09587264825947904 \t 0.13181266935649952\n",
      "--- 0.774 seconds in epoch ---\n",
      "\n",
      "Epoch 57\n",
      "mean training loss\t 0.1655157626163764\n",
      "epoch 57 '\n",
      "MSE for train and val\t 0.07738433740919982 \t 0.1140688035151071\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 58\n",
      "mean training loss\t 0.164684074897258\n",
      "epoch 58 '\n",
      "MSE for train and val\t 0.07731140062129523 \t 0.11402242469960555\n",
      "--- 0.746 seconds in epoch ---\n",
      "\n",
      "Epoch 59\n",
      "mean training loss\t 0.16452297338696778\n",
      "epoch 59 '\n",
      "MSE for train and val\t 0.07740747426900271 \t 0.11306633825610911\n",
      "--- 0.784 seconds in epoch ---\n",
      "\n",
      "Epoch 60\n",
      "mean training loss\t 0.16404803329315343\n",
      "epoch 60 '\n",
      "MSE for train and val\t 0.07679253279008412 \t 0.11267972417540369\n",
      "--- 0.792 seconds in epoch ---\n",
      "\n",
      "Epoch 61\n",
      "mean training loss\t 0.16337418773623763\n",
      "epoch 61 '\n",
      "MSE for train and val\t 0.07620806496932041 \t 0.11280271118949285\n",
      "--- 0.748 seconds in epoch ---\n",
      "\n",
      "Epoch 62\n",
      "mean training loss\t 0.1634945396517144\n",
      "epoch 62 '\n",
      "MSE for train and val\t 0.07595535879346571 \t 0.11165905456443817\n",
      "--- 0.784 seconds in epoch ---\n",
      "\n",
      "Epoch 63\n",
      "mean training loss\t 0.16254063627270401\n",
      "epoch 63 '\n",
      "MSE for train and val\t 0.07505835772793218 \t 0.11222250890765297\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 64\n",
      "mean training loss\t 0.16187970711559546\n",
      "epoch 64 '\n",
      "MSE for train and val\t 0.07445732358564054 \t 0.11178731886657531\n",
      "--- 0.769 seconds in epoch ---\n",
      "\n",
      "Epoch 65\n",
      "mean training loss\t 0.16172017034937125\n",
      "epoch 65 '\n",
      "MSE for train and val\t 0.07973362326494987 \t 0.11750644037662501\n",
      "--- 0.8 seconds in epoch ---\n",
      "\n",
      "Epoch 66\n",
      "mean training loss\t 0.16163823269918318\n",
      "epoch 66 '\n",
      "MSE for train and val\t 0.07659759445410719 \t 0.11421755917062018\n",
      "--- 0.785 seconds in epoch ---\n",
      "\n",
      "Epoch 67\n",
      "mean training loss\t 0.1610352307802341\n",
      "epoch 67 '\n",
      "MSE for train and val\t 0.0748476549623838 \t 0.11226784203943928\n",
      "--- 0.734 seconds in epoch ---\n",
      "\n",
      "Epoch 68\n",
      "mean training loss\t 0.16053385363250483\n",
      "epoch 68 '\n",
      "MSE for train and val\t 0.07355063092319664 \t 0.11077838367474849\n",
      "--- 0.827 seconds in epoch ---\n",
      "\n",
      "Epoch 69\n",
      "mean training loss\t 0.16049514178369867\n",
      "epoch 69 '\n",
      "MSE for train and val\t 0.09570963926890715 \t 0.13105986176769122\n",
      "--- 0.793 seconds in epoch ---\n",
      "\n",
      "Epoch 70\n",
      "mean training loss\t 0.1599074383495284\n",
      "epoch 70 '\n",
      "MSE for train and val\t 0.07436800114519437 \t 0.11160876019933151\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 71\n",
      "mean training loss\t 0.15969191025026508\n",
      "epoch 71 '\n",
      "MSE for train and val\t 0.07620805464204537 \t 0.1120622063259916\n",
      "--- 0.784 seconds in epoch ---\n",
      "\n",
      "Epoch 72\n",
      "mean training loss\t 0.1590633517406026\n",
      "epoch 72 '\n",
      "MSE for train and val\t 0.07284948524615342 \t 0.11015093940356005\n",
      "--- 0.737 seconds in epoch ---\n",
      "\n",
      "Epoch 73\n",
      "mean training loss\t 0.15860411747068656\n",
      "epoch 73 '\n",
      "MSE for train and val\t 0.07832873579275255 \t 0.11731212271039687\n",
      "--- 0.764 seconds in epoch ---\n",
      "\n",
      "Epoch 74\n",
      "mean training loss\t 0.1581362498343968\n",
      "epoch 74 '\n",
      "MSE for train and val\t 0.07272337373348252 \t 0.11066728775139308\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 75\n",
      "mean training loss\t 0.1577427932717761\n",
      "epoch 75 '\n",
      "MSE for train and val\t 0.07239175086340477 \t 0.10980279302886145\n",
      "--- 0.806 seconds in epoch ---\n",
      "\n",
      "Epoch 76\n",
      "mean training loss\t 0.1574912006737756\n",
      "epoch 76 '\n",
      "MSE for train and val\t 0.075643865017592 \t 0.1147013676294425\n",
      "--- 0.746 seconds in epoch ---\n",
      "\n",
      "Epoch 77\n",
      "mean training loss\t 0.1569820820552404\n",
      "epoch 77 '\n",
      "MSE for train and val\t 0.07537511897299569 \t 0.11239344383879458\n",
      "--- 0.747 seconds in epoch ---\n",
      "\n",
      "Epoch 78\n",
      "mean training loss\t 0.15646891752715972\n",
      "epoch 78 '\n",
      "MSE for train and val\t 0.07120245384679677 \t 0.10953764793612704\n",
      "--- 0.781 seconds in epoch ---\n",
      "\n",
      "Epoch 79\n",
      "mean training loss\t 0.15657502105001544\n",
      "epoch 79 '\n",
      "MSE for train and val\t 0.0718964206046932 \t 0.11030749424292176\n",
      "--- 0.806 seconds in epoch ---\n",
      "\n",
      "Epoch 80\n",
      "mean training loss\t 0.15571896431387447\n",
      "epoch 80 '\n",
      "MSE for train and val\t 0.07742437009486491 \t 0.11557007227129852\n",
      "--- 0.763 seconds in epoch ---\n",
      "\n",
      "Epoch 81\n",
      "mean training loss\t 0.15542931175622784\n",
      "epoch 81 '\n",
      "MSE for train and val\t 0.07114253944129185 \t 0.10972078468404207\n",
      "--- 0.76 seconds in epoch ---\n",
      "\n",
      "Epoch 82\n",
      "mean training loss\t 0.15549127215244732\n",
      "epoch 82 '\n",
      "MSE for train and val\t 0.08793254498572746 \t 0.12767032726157732\n",
      "--- 0.822 seconds in epoch ---\n",
      "\n",
      "Epoch 83\n",
      "mean training loss\t 0.15513114741102593\n",
      "epoch 83 '\n",
      "MSE for train and val\t 0.07486992995324357 \t 0.11327369269397473\n",
      "--- 0.763 seconds in epoch ---\n",
      "\n",
      "Epoch 84\n",
      "mean training loss\t 0.15462872285823354\n",
      "epoch 84 '\n",
      "MSE for train and val\t 0.06982181289992226 \t 0.10872395978735386\n",
      "--- 0.762 seconds in epoch ---\n",
      "\n",
      "Epoch 85\n",
      "mean training loss\t 0.15443045619081278\n",
      "epoch 85 '\n",
      "MSE for train and val\t 0.06992583062604443 \t 0.10880165934322196\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 86\n",
      "mean training loss\t 0.15415947803708374\n",
      "epoch 86 '\n",
      "MSE for train and val\t 0.07321690144202377 \t 0.11242993209386315\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 87\n",
      "mean training loss\t 0.1539449841516917\n",
      "epoch 87 '\n",
      "MSE for train and val\t 0.07120089478989103 \t 0.1094287775304985\n",
      "--- 0.796 seconds in epoch ---\n",
      "\n",
      "Epoch 88\n",
      "mean training loss\t 0.15337292893988186\n",
      "epoch 88 '\n",
      "MSE for train and val\t 0.06978179461469579 \t 0.10908751156789782\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 89\n",
      "mean training loss\t 0.15292421464548736\n",
      "epoch 89 '\n",
      "MSE for train and val\t 0.07481150371796227 \t 0.11555572023229765\n",
      "--- 0.8 seconds in epoch ---\n",
      "\n",
      "Epoch 90\n",
      "mean training loss\t 0.1530823703672065\n",
      "epoch 90 '\n",
      "MSE for train and val\t 0.07165562772961108 \t 0.11078910146861506\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 91\n",
      "mean training loss\t 0.15251134189425922\n",
      "epoch 91 '\n",
      "MSE for train and val\t 0.07002213829487484 \t 0.11072330530236647\n",
      "--- 0.804 seconds in epoch ---\n",
      "\n",
      "Epoch 92\n",
      "mean training loss\t 0.1522316042272771\n",
      "epoch 92 '\n",
      "MSE for train and val\t 0.07971051435118161 \t 0.11643167760775586\n",
      "--- 0.71 seconds in epoch ---\n",
      "\n",
      "Epoch 93\n",
      "mean training loss\t 0.15229048716728805\n",
      "epoch 93 '\n",
      "MSE for train and val\t 0.07258694706070828 \t 0.1121791220798829\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 94\n",
      "mean training loss\t 0.15199501810015226\n",
      "epoch 94 '\n",
      "MSE for train and val\t 0.07430734927649861 \t 0.11220407849315334\n",
      "--- 0.774 seconds in epoch ---\n",
      "\n",
      "Epoch 95\n",
      "mean training loss\t 0.1515606513277429\n",
      "epoch 95 '\n",
      "MSE for train and val\t 0.06687211715194068 \t 0.10644050185667696\n",
      "--- 0.754 seconds in epoch ---\n",
      "\n",
      "Epoch 96\n",
      "mean training loss\t 0.151240010950409\n",
      "epoch 96 '\n",
      "MSE for train and val\t 0.06858110371200114 \t 0.1072949813296529\n",
      "--- 0.757 seconds in epoch ---\n",
      "\n",
      "Epoch 97\n",
      "mean training loss\t 0.15108156541331869\n",
      "epoch 97 '\n",
      "MSE for train and val\t 0.06744455995354762 \t 0.10768463615320314\n",
      "--- 0.768 seconds in epoch ---\n",
      "\n",
      "Epoch 98\n",
      "mean training loss\t 0.1506485384751539\n",
      "epoch 98 '\n",
      "MSE for train and val\t 0.06664078923014524 \t 0.10588138985990812\n",
      "--- 0.781 seconds in epoch ---\n",
      "\n",
      "Epoch 99\n",
      "mean training loss\t 0.15018366223475973\n",
      "epoch 99 '\n",
      "MSE for train and val\t 0.06686347701518511 \t 0.10581368364946027\n",
      "--- 0.771 seconds in epoch ---\n",
      "\n",
      "Epoch 100\n",
      "mean training loss\t 0.15023819816405656\n",
      "epoch 100 '\n",
      "MSE for train and val\t 0.07104960090598678 \t 0.1115660539638992\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 101\n",
      "mean training loss\t 0.1499066857285187\n",
      "epoch 101 '\n",
      "MSE for train and val\t 0.06821140458552548 \t 0.1077584604761397\n",
      "--- 0.749 seconds in epoch ---\n",
      "\n",
      "Epoch 102\n",
      "mean training loss\t 0.14993752690612291\n",
      "epoch 102 '\n",
      "MSE for train and val\t 0.07995771850621292 \t 0.11946566203954369\n",
      "--- 0.781 seconds in epoch ---\n",
      "\n",
      "Epoch 103\n",
      "mean training loss\t 0.14977724349401036\n",
      "epoch 103 '\n",
      "MSE for train and val\t 0.07385736748749155 \t 0.11452825577667756\n",
      "--- 0.815 seconds in epoch ---\n",
      "\n",
      "Epoch 104\n",
      "mean training loss\t 0.1493017060590572\n",
      "epoch 104 '\n",
      "MSE for train and val\t 0.07191076047239135 \t 0.11171438295780174\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 105\n",
      "mean training loss\t 0.14893302656099444\n",
      "epoch 105 '\n",
      "MSE for train and val\t 0.07144191629550858 \t 0.11124349749481356\n",
      "--- 0.742 seconds in epoch ---\n",
      "\n",
      "Epoch 106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.14862106904143194\n",
      "epoch 106 '\n",
      "MSE for train and val\t 0.06600350279606888 \t 0.10692444117605923\n",
      "--- 0.801 seconds in epoch ---\n",
      "\n",
      "Epoch 107\n",
      "mean training loss\t 0.14844263144692438\n",
      "epoch 107 '\n",
      "MSE for train and val\t 0.06662860748174969 \t 0.10628894283990083\n",
      "--- 0.749 seconds in epoch ---\n",
      "\n",
      "Epoch 108\n",
      "mean training loss\t 0.14845687346380265\n",
      "epoch 108 '\n",
      "MSE for train and val\t 0.06732149922044584 \t 0.10810437063835666\n",
      "--- 0.801 seconds in epoch ---\n",
      "\n",
      "Epoch 109\n",
      "mean training loss\t 0.14813384811897748\n",
      "epoch 109 '\n",
      "MSE for train and val\t 0.06952298449945393 \t 0.1103924907237416\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 110\n",
      "mean training loss\t 0.1477981105202534\n",
      "epoch 110 '\n",
      "MSE for train and val\t 0.0680225590475049 \t 0.10851621102699217\n",
      "--- 0.798 seconds in epoch ---\n",
      "\n",
      "Epoch 111\n",
      "mean training loss\t 0.14757597900804925\n",
      "epoch 111 '\n",
      "MSE for train and val\t 0.06808341479739387 \t 0.10885694912957926\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 112\n",
      "mean training loss\t 0.14741087709782552\n",
      "epoch 112 '\n",
      "MSE for train and val\t 0.06578963088591384 \t 0.10649290177568944\n",
      "--- 0.769 seconds in epoch ---\n",
      "\n",
      "Epoch 113\n",
      "mean training loss\t 0.14707969273211527\n",
      "epoch 113 '\n",
      "MSE for train and val\t 0.06477903457676866 \t 0.10557798620015112\n",
      "--- 0.782 seconds in epoch ---\n",
      "\n",
      "Epoch 114\n",
      "mean training loss\t 0.14680254398310771\n",
      "epoch 114 '\n",
      "MSE for train and val\t 0.06526651969356433 \t 0.1049624512606945\n",
      "--- 0.734 seconds in epoch ---\n",
      "\n",
      "Epoch 115\n",
      "mean training loss\t 0.14671032079419152\n",
      "epoch 115 '\n",
      "MSE for train and val\t 0.06565736714347715 \t 0.10737746013413306\n",
      "--- 0.777 seconds in epoch ---\n",
      "\n",
      "Epoch 116\n",
      "mean training loss\t 0.1467269663195141\n",
      "epoch 116 '\n",
      "MSE for train and val\t 0.0639339337891691 \t 0.10445469670535644\n",
      "--- 0.757 seconds in epoch ---\n",
      "\n",
      "Epoch 117\n",
      "mean training loss\t 0.14634887185253082\n",
      "epoch 117 '\n",
      "MSE for train and val\t 0.07373181629516287 \t 0.116124621841315\n",
      "--- 0.79 seconds in epoch ---\n",
      "\n",
      "Epoch 118\n",
      "mean training loss\t 0.1464866155727965\n",
      "epoch 118 '\n",
      "MSE for train and val\t 0.07135485368197682 \t 0.11261233322321224\n",
      "--- 0.752 seconds in epoch ---\n",
      "\n",
      "Epoch 119\n",
      "mean training loss\t 0.14631937537036957\n",
      "epoch 119 '\n",
      "MSE for train and val\t 0.06512237235448942 \t 0.10461173194535264\n",
      "--- 0.796 seconds in epoch ---\n",
      "\n",
      "Epoch 120\n",
      "mean training loss\t 0.14583130969864422\n",
      "epoch 120 '\n",
      "MSE for train and val\t 0.06700490518644316 \t 0.10718078799290738\n",
      "--- 0.737 seconds in epoch ---\n",
      "\n",
      "Epoch 121\n",
      "mean training loss\t 0.1453417974417327\n",
      "epoch 121 '\n",
      "MSE for train and val\t 0.0638670871266 \t 0.10464611281113648\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 122\n",
      "mean training loss\t 0.14543913163122585\n",
      "epoch 122 '\n",
      "MSE for train and val\t 0.0661584901844533 \t 0.10848682820943563\n",
      "--- 0.789 seconds in epoch ---\n",
      "\n",
      "Epoch 123\n",
      "mean training loss\t 0.1449238410005804\n",
      "epoch 123 '\n",
      "MSE for train and val\t 0.0658524558172294 \t 0.10607730890978932\n",
      "--- 0.757 seconds in epoch ---\n",
      "\n",
      "Epoch 124\n",
      "mean training loss\t 0.14497229950349838\n",
      "epoch 124 '\n",
      "MSE for train and val\t 0.063727089836251 \t 0.10471933324462049\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 125\n",
      "mean training loss\t 0.1449514105671742\n",
      "epoch 125 '\n",
      "MSE for train and val\t 0.06944455868566175 \t 0.11021335913891125\n",
      "--- 0.791 seconds in epoch ---\n",
      "\n",
      "Epoch 126\n",
      "mean training loss\t 0.14463495223737155\n",
      "epoch 126 '\n",
      "MSE for train and val\t 0.06308980885911254 \t 0.10453483605291072\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 127\n",
      "mean training loss\t 0.1440576358896787\n",
      "epoch 127 '\n",
      "MSE for train and val\t 0.06285668042402114 \t 0.1038276818373491\n",
      "--- 0.78 seconds in epoch ---\n",
      "\n",
      "Epoch 128\n",
      "mean training loss\t 0.14408435794662255\n",
      "epoch 128 '\n",
      "MSE for train and val\t 0.06248918038829579 \t 0.10352964139731642\n",
      "--- 0.789 seconds in epoch ---\n",
      "\n",
      "Epoch 129\n",
      "mean training loss\t 0.1448330715542934\n",
      "epoch 129 '\n",
      "MSE for train and val\t 0.09260217939512488 \t 0.13137076060549635\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 130\n",
      "mean training loss\t 0.14409353984672515\n",
      "epoch 130 '\n",
      "MSE for train and val\t 0.062032937750295565 \t 0.10328927267129938\n",
      "--- 0.798 seconds in epoch ---\n",
      "\n",
      "Epoch 131\n",
      "mean training loss\t 0.14343467242405064\n",
      "epoch 131 '\n",
      "MSE for train and val\t 0.06293781971561888 \t 0.1044696209400814\n",
      "--- 0.808 seconds in epoch ---\n",
      "\n",
      "Epoch 132\n",
      "mean training loss\t 0.1432344050192442\n",
      "epoch 132 '\n",
      "MSE for train and val\t 0.06292979727145513 \t 0.10428458678473063\n",
      "--- 0.782 seconds in epoch ---\n",
      "\n",
      "Epoch 133\n",
      "mean training loss\t 0.143256470829737\n",
      "epoch 133 '\n",
      "MSE for train and val\t 0.06426416575218077 \t 0.10534626848147517\n",
      "--- 0.81 seconds in epoch ---\n",
      "\n",
      "Epoch 134\n",
      "mean training loss\t 0.14356678597262648\n",
      "epoch 134 '\n",
      "MSE for train and val\t 0.06817649210254073 \t 0.11181471491719484\n",
      "--- 0.754 seconds in epoch ---\n",
      "\n",
      "Epoch 135\n",
      "mean training loss\t 0.14265243337779748\n",
      "epoch 135 '\n",
      "MSE for train and val\t 0.06159554101972879 \t 0.10407675748268355\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 136\n",
      "mean training loss\t 0.14289994391261554\n",
      "epoch 136 '\n",
      "MSE for train and val\t 0.06195972298689 \t 0.10411279841261188\n",
      "--- 0.803 seconds in epoch ---\n",
      "\n",
      "Epoch 137\n",
      "mean training loss\t 0.1423604334964127\n",
      "epoch 137 '\n",
      "MSE for train and val\t 0.06551270250219836 \t 0.10758550263128902\n",
      "--- 0.756 seconds in epoch ---\n",
      "\n",
      "Epoch 138\n",
      "mean training loss\t 0.14253217761145262\n",
      "epoch 138 '\n",
      "MSE for train and val\t 0.061643450913797124 \t 0.10328516871070524\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 139\n",
      "mean training loss\t 0.14190720808310586\n",
      "epoch 139 '\n",
      "MSE for train and val\t 0.06106426608491271 \t 0.10273208643072129\n",
      "--- 0.738 seconds in epoch ---\n",
      "\n",
      "Epoch 140\n",
      "mean training loss\t 0.1428968219483485\n",
      "epoch 140 '\n",
      "MSE for train and val\t 0.07000313851615789 \t 0.11081829734033721\n",
      "--- 0.781 seconds in epoch ---\n",
      "\n",
      "Epoch 141\n",
      "mean training loss\t 0.14186472010905626\n",
      "epoch 141 '\n",
      "MSE for train and val\t 0.06075464670823044 \t 0.103014391752579\n",
      "--- 0.781 seconds in epoch ---\n",
      "\n",
      "Epoch 142\n",
      "mean training loss\t 0.14165077742005958\n",
      "epoch 142 '\n",
      "MSE for train and val\t 0.06057590540796483 \t 0.10218671916088642\n",
      "--- 0.769 seconds in epoch ---\n",
      "\n",
      "Epoch 143\n",
      "mean training loss\t 0.14121200102763098\n",
      "epoch 143 '\n",
      "MSE for train and val\t 0.06638137868106192 \t 0.10968324739014168\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 144\n",
      "mean training loss\t 0.14112163989758883\n",
      "epoch 144 '\n",
      "MSE for train and val\t 0.06076827040797299 \t 0.10273209591041874\n",
      "--- 0.843 seconds in epoch ---\n",
      "\n",
      "Epoch 145\n",
      "mean training loss\t 0.1409179578306245\n",
      "epoch 145 '\n",
      "MSE for train and val\t 0.06056608832912224 \t 0.10191717080799738\n",
      "--- 0.764 seconds in epoch ---\n",
      "\n",
      "Epoch 146\n",
      "mean training loss\t 0.1408258506997687\n",
      "epoch 146 '\n",
      "MSE for train and val\t 0.06110638725653616 \t 0.10386967294426129\n",
      "--- 0.793 seconds in epoch ---\n",
      "\n",
      "Epoch 147\n",
      "mean training loss\t 0.14109735444920962\n",
      "epoch 147 '\n",
      "MSE for train and val\t 0.06224405396437522 \t 0.104977760198962\n",
      "--- 0.799 seconds in epoch ---\n",
      "\n",
      "Epoch 148\n",
      "mean training loss\t 0.14056113682320862\n",
      "epoch 148 '\n",
      "MSE for train and val\t 0.06217349704155936 \t 0.10502910401052261\n",
      "--- 0.764 seconds in epoch ---\n",
      "\n",
      "Epoch 149\n",
      "mean training loss\t 0.14036056890350873\n",
      "epoch 149 '\n",
      "MSE for train and val\t 0.06207116357208134 \t 0.10341257521598722\n",
      "--- 0.744 seconds in epoch ---\n",
      "\n",
      "Epoch 150\n",
      "mean training loss\t 0.14038139137088276\n",
      "epoch 150 '\n",
      "MSE for train and val\t 0.060399429736672804 \t 0.10247996672125553\n",
      "--- 0.771 seconds in epoch ---\n",
      "\n",
      "Epoch 151\n",
      "mean training loss\t 0.14019544845233198\n",
      "epoch 151 '\n",
      "MSE for train and val\t 0.061087192799324924 \t 0.10382894364095299\n",
      "--- 0.768 seconds in epoch ---\n",
      "\n",
      "Epoch 152\n",
      "mean training loss\t 0.13993816268248635\n",
      "epoch 152 '\n",
      "MSE for train and val\t 0.059662643321419524 \t 0.1017909646597941\n",
      "--- 0.747 seconds in epoch ---\n",
      "\n",
      "Epoch 153\n",
      "mean training loss\t 0.140062313563511\n",
      "epoch 153 '\n",
      "MSE for train and val\t 0.06070286036977649 \t 0.10286076402324254\n",
      "--- 0.757 seconds in epoch ---\n",
      "\n",
      "Epoch 154\n",
      "mean training loss\t 0.13969400744946275\n",
      "epoch 154 '\n",
      "MSE for train and val\t 0.06229203362483551 \t 0.10495852172052297\n",
      "--- 0.779 seconds in epoch ---\n",
      "\n",
      "Epoch 155\n",
      "mean training loss\t 0.13943468461759756\n",
      "epoch 155 '\n",
      "MSE for train and val\t 0.060217546667782625 \t 0.10294295018977581\n",
      "--- 0.746 seconds in epoch ---\n",
      "\n",
      "Epoch 156\n",
      "mean training loss\t 0.13934922618944137\n",
      "epoch 156 '\n",
      "MSE for train and val\t 0.05981627724671149 \t 0.10315411890968956\n",
      "--- 0.784 seconds in epoch ---\n",
      "\n",
      "Epoch 157\n",
      "mean training loss\t 0.13945932139138706\n",
      "epoch 157 '\n",
      "MSE for train and val\t 0.060601394864466473 \t 0.10327905280399181\n",
      "--- 0.795 seconds in epoch ---\n",
      "\n",
      "Epoch 158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.13921838151138338\n",
      "epoch 158 '\n",
      "MSE for train and val\t 0.06562664923726114 \t 0.109405187987065\n",
      "--- 0.804 seconds in epoch ---\n",
      "\n",
      "Epoch 159\n",
      "mean training loss\t 0.13903097742405093\n",
      "epoch 159 '\n",
      "MSE for train and val\t 0.06123542930633144 \t 0.10460290090384468\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 160\n",
      "mean training loss\t 0.13896128979862712\n",
      "epoch 160 '\n",
      "MSE for train and val\t 0.06168516656005961 \t 0.10450801479771986\n",
      "--- 0.762 seconds in epoch ---\n",
      "\n",
      "Epoch 161\n",
      "mean training loss\t 0.13889854594332274\n",
      "epoch 161 '\n",
      "MSE for train and val\t 0.060368156175763354 \t 0.1030504281539425\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 162\n",
      "mean training loss\t 0.1384786671546639\n",
      "epoch 162 '\n",
      "MSE for train and val\t 0.0598877636306508 \t 0.10250265929644559\n",
      "--- 0.779 seconds in epoch ---\n",
      "\n",
      "Epoch 163\n",
      "mean training loss\t 0.1383821721448273\n",
      "epoch 163 '\n",
      "MSE for train and val\t 0.05988098525300393 \t 0.10305144648558327\n",
      "--- 0.76 seconds in epoch ---\n",
      "\n",
      "Epoch 164\n",
      "mean training loss\t 0.13819487121750096\n",
      "epoch 164 '\n",
      "MSE for train and val\t 0.05905135133372936 \t 0.10237701626024767\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 165\n",
      "mean training loss\t 0.13833535567658847\n",
      "epoch 165 '\n",
      "MSE for train and val\t 0.060082413217075956 \t 0.1028481286211905\n",
      "--- 0.815 seconds in epoch ---\n",
      "\n",
      "Epoch 166\n",
      "mean training loss\t 0.13838266740079785\n",
      "epoch 166 '\n",
      "MSE for train and val\t 0.06051331049603003 \t 0.10250781761811179\n",
      "--- 0.77 seconds in epoch ---\n",
      "\n",
      "Epoch 167\n",
      "mean training loss\t 0.1378374221139267\n",
      "epoch 167 '\n",
      "MSE for train and val\t 0.05891896314382503 \t 0.10192353149906788\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 168\n",
      "mean training loss\t 0.13776691558419682\n",
      "epoch 168 '\n",
      "MSE for train and val\t 0.059802943754666354 \t 0.10213808373109728\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 169\n",
      "mean training loss\t 0.13775722372238752\n",
      "epoch 169 '\n",
      "MSE for train and val\t 0.05830679740460557 \t 0.10149225059669126\n",
      "--- 1.029 seconds in epoch ---\n",
      "\n",
      "Epoch 170\n",
      "mean training loss\t 0.13742213583871968\n",
      "epoch 170 '\n",
      "MSE for train and val\t 0.06046711351775578 \t 0.10303249769166972\n",
      "--- 0.825 seconds in epoch ---\n",
      "\n",
      "Epoch 171\n",
      "mean training loss\t 0.13754235901793496\n",
      "epoch 171 '\n",
      "MSE for train and val\t 0.05990206818416504 \t 0.1032653418140615\n",
      "--- 0.805 seconds in epoch ---\n",
      "\n",
      "Epoch 172\n",
      "mean training loss\t 0.13721928796807273\n",
      "epoch 172 '\n",
      "MSE for train and val\t 0.057720993528158474 \t 0.1010684204667656\n",
      "--- 0.818 seconds in epoch ---\n",
      "\n",
      "Epoch 173\n",
      "mean training loss\t 0.1370130080912934\n",
      "epoch 173 '\n",
      "MSE for train and val\t 0.06139830463792253 \t 0.10622962124878854\n",
      "--- 0.805 seconds in epoch ---\n",
      "\n",
      "Epoch 174\n",
      "mean training loss\t 0.13685407736262337\n",
      "epoch 174 '\n",
      "MSE for train and val\t 0.05857818739825617 \t 0.10181336466696253\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 175\n",
      "mean training loss\t 0.1371550510164167\n",
      "epoch 175 '\n",
      "MSE for train and val\t 0.06600751855358342 \t 0.10956933768994155\n",
      "--- 0.809 seconds in epoch ---\n",
      "\n",
      "Epoch 176\n",
      "mean training loss\t 0.1367914673979165\n",
      "epoch 176 '\n",
      "MSE for train and val\t 0.05715348275262841 \t 0.10072450666866628\n",
      "--- 0.809 seconds in epoch ---\n",
      "\n",
      "Epoch 177\n",
      "mean training loss\t 0.1363942754806065\n",
      "epoch 177 '\n",
      "MSE for train and val\t 0.05750633967488409 \t 0.1009984408291749\n",
      "--- 0.789 seconds in epoch ---\n",
      "\n",
      "Epoch 178\n",
      "mean training loss\t 0.13649983213084643\n",
      "epoch 178 '\n",
      "MSE for train and val\t 0.05823014221918538 \t 0.10174467791992399\n",
      "--- 0.779 seconds in epoch ---\n",
      "\n",
      "Epoch 179\n",
      "mean training loss\t 0.1361393684246501\n",
      "epoch 179 '\n",
      "MSE for train and val\t 0.05800976197968893 \t 0.10177170455194585\n",
      "--- 0.74 seconds in epoch ---\n",
      "\n",
      "Epoch 180\n",
      "mean training loss\t 0.13629440855784494\n",
      "epoch 180 '\n",
      "MSE for train and val\t 0.06116381882239937 \t 0.10387427133118403\n",
      "--- 0.765 seconds in epoch ---\n",
      "\n",
      "Epoch 181\n",
      "mean training loss\t 0.1361676286722793\n",
      "epoch 181 '\n",
      "MSE for train and val\t 0.0624482701672718 \t 0.10492860296988009\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 182\n",
      "mean training loss\t 0.13595476184712083\n",
      "epoch 182 '\n",
      "MSE for train and val\t 0.05813008929593091 \t 0.10144983199367251\n",
      "--- 0.754 seconds in epoch ---\n",
      "\n",
      "Epoch 183\n",
      "mean training loss\t 0.13571767592039263\n",
      "epoch 183 '\n",
      "MSE for train and val\t 0.05702994735803616 \t 0.10097231419462413\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 184\n",
      "mean training loss\t 0.13545439038120333\n",
      "epoch 184 '\n",
      "MSE for train and val\t 0.05853337744225273 \t 0.10225953808703143\n",
      "--- 0.757 seconds in epoch ---\n",
      "\n",
      "Epoch 185\n",
      "mean training loss\t 0.1354940626953469\n",
      "epoch 185 '\n",
      "MSE for train and val\t 0.0584294490038838 \t 0.10203951516327966\n",
      "--- 0.775 seconds in epoch ---\n",
      "\n",
      "Epoch 186\n",
      "mean training loss\t 0.1356842522249847\n",
      "epoch 186 '\n",
      "MSE for train and val\t 0.05931079479687066 \t 0.10258762134639461\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 187\n",
      "mean training loss\t 0.1351387386683558\n",
      "epoch 187 '\n",
      "MSE for train and val\t 0.057889442057783076 \t 0.10145839991623687\n",
      "--- 0.762 seconds in epoch ---\n",
      "\n",
      "Epoch 188\n",
      "mean training loss\t 0.13499190309008613\n",
      "epoch 188 '\n",
      "MSE for train and val\t 0.05673779070958846 \t 0.10077290596741063\n",
      "--- 0.796 seconds in epoch ---\n",
      "\n",
      "Epoch 189\n",
      "mean training loss\t 0.13523603111016946\n",
      "epoch 189 '\n",
      "MSE for train and val\t 0.057826886061901846 \t 0.1020401953483855\n",
      "--- 0.802 seconds in epoch ---\n",
      "\n",
      "Epoch 190\n",
      "mean training loss\t 0.13469755283144655\n",
      "epoch 190 '\n",
      "MSE for train and val\t 0.05778440204955212 \t 0.10130498901278148\n",
      "--- 0.75 seconds in epoch ---\n",
      "\n",
      "Epoch 191\n",
      "mean training loss\t 0.13497882011483928\n",
      "epoch 191 '\n",
      "MSE for train and val\t 0.06170202483772768 \t 0.10713961059796683\n",
      "--- 0.76 seconds in epoch ---\n",
      "\n",
      "Epoch 192\n",
      "mean training loss\t 0.13486975751450805\n",
      "epoch 192 '\n",
      "MSE for train and val\t 0.05772939826710262 \t 0.10190689040186883\n",
      "--- 0.786 seconds in epoch ---\n",
      "\n",
      "Epoch 193\n",
      "mean training loss\t 0.13498389857714294\n",
      "epoch 193 '\n",
      "MSE for train and val\t 0.059178068832928754 \t 0.10204428532198051\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 194\n",
      "mean training loss\t 0.13441684167893206\n",
      "epoch 194 '\n",
      "MSE for train and val\t 0.05708882044603223 \t 0.1021884676437651\n",
      "--- 0.804 seconds in epoch ---\n",
      "\n",
      "Epoch 195\n",
      "mean training loss\t 0.13449240119242278\n",
      "epoch 195 '\n",
      "MSE for train and val\t 0.058513327854430565 \t 0.10323817738018691\n",
      "--- 0.747 seconds in epoch ---\n",
      "\n",
      "Epoch 196\n",
      "mean training loss\t 0.1344852131898286\n",
      "epoch 196 '\n",
      "MSE for train and val\t 0.0568815350183513 \t 0.10037148022571922\n",
      "--- 0.804 seconds in epoch ---\n",
      "\n",
      "Epoch 197\n",
      "mean training loss\t 0.1339396511922117\n",
      "epoch 197 '\n",
      "MSE for train and val\t 0.05695110181776483 \t 0.10167119180530815\n",
      "--- 0.791 seconds in epoch ---\n",
      "\n",
      "Epoch 198\n",
      "mean training loss\t 0.13404048637288515\n",
      "epoch 198 '\n",
      "MSE for train and val\t 0.058870178009514024 \t 0.10190766682279978\n",
      "--- 0.799 seconds in epoch ---\n",
      "\n",
      "Epoch 199\n",
      "mean training loss\t 0.13378377251449178\n",
      "epoch 199 '\n",
      "MSE for train and val\t 0.05630621452822653 \t 0.10092254853990902\n",
      "--- 0.78 seconds in epoch ---\n",
      "\n",
      "Epoch 200\n",
      "mean training loss\t 0.13380474944095141\n",
      "epoch 200 '\n",
      "MSE for train and val\t 0.0570736847053545 \t 0.10115875002998378\n",
      "--- 0.78 seconds in epoch ---\n",
      "\n",
      "Epoch 201\n",
      "mean training loss\t 0.13411049031820454\n",
      "epoch 201 '\n",
      "MSE for train and val\t 0.061700887966525196 \t 0.10554466111417164\n",
      "--- 0.741 seconds in epoch ---\n",
      "\n",
      "Epoch 202\n",
      "mean training loss\t 0.13353197931266222\n",
      "epoch 202 '\n",
      "MSE for train and val\t 0.055756234916784775 \t 0.09991992947496879\n",
      "--- 0.755 seconds in epoch ---\n",
      "\n",
      "Epoch 203\n",
      "mean training loss\t 0.1333933528818068\n",
      "epoch 203 '\n",
      "MSE for train and val\t 0.0564859588942731 \t 0.10146921497225872\n",
      "--- 0.774 seconds in epoch ---\n",
      "\n",
      "Epoch 204\n",
      "mean training loss\t 0.13310763562311892\n",
      "epoch 204 '\n",
      "MSE for train and val\t 0.05538118869593105 \t 0.0999444833072096\n",
      "--- 0.787 seconds in epoch ---\n",
      "\n",
      "Epoch 205\n",
      "mean training loss\t 0.13330627066678688\n",
      "epoch 205 '\n",
      "MSE for train and val\t 0.05568876212111166 \t 0.1000112573326589\n",
      "--- 0.818 seconds in epoch ---\n",
      "\n",
      "Epoch 206\n",
      "mean training loss\t 0.13302155014921407\n",
      "epoch 206 '\n",
      "MSE for train and val\t 0.055772352608984596 \t 0.09984623042976387\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 207\n",
      "mean training loss\t 0.13287791985957348\n",
      "epoch 207 '\n",
      "MSE for train and val\t 0.05736523433538126 \t 0.10124211297991415\n",
      "--- 0.733 seconds in epoch ---\n",
      "\n",
      "Epoch 208\n",
      "mean training loss\t 0.1328428636564583\n",
      "epoch 208 '\n",
      "MSE for train and val\t 0.056478485006893175 \t 0.10206317759386524\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 209\n",
      "mean training loss\t 0.1329392874582869\n",
      "epoch 209 '\n",
      "MSE for train and val\t 0.056794252929745555 \t 0.10042838288045224\n",
      "--- 0.762 seconds in epoch ---\n",
      "\n",
      "Epoch 210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.13259520516043805\n",
      "epoch 210 '\n",
      "MSE for train and val\t 0.055177495212888554 \t 0.09990244734832217\n",
      "--- 0.822 seconds in epoch ---\n",
      "\n",
      "Epoch 211\n",
      "mean training loss\t 0.13250397536598268\n",
      "epoch 211 '\n",
      "MSE for train and val\t 0.055397827535505595 \t 0.100067992394752\n",
      "--- 0.793 seconds in epoch ---\n",
      "\n",
      "Epoch 212\n",
      "mean training loss\t 0.13248725069350883\n",
      "epoch 212 '\n",
      "MSE for train and val\t 0.05617546645405105 \t 0.10040217183826898\n",
      "--- 0.811 seconds in epoch ---\n",
      "\n",
      "Epoch 213\n",
      "mean training loss\t 0.1322903780175037\n",
      "epoch 213 '\n",
      "MSE for train and val\t 0.05594135274512708 \t 0.10082197521019046\n",
      "--- 0.792 seconds in epoch ---\n",
      "\n",
      "Epoch 214\n",
      "mean training loss\t 0.13236528517769985\n",
      "epoch 214 '\n",
      "MSE for train and val\t 0.05516333418745923 \t 0.09950661250528668\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 215\n",
      "mean training loss\t 0.13225418671232755\n",
      "epoch 215 '\n",
      "MSE for train and val\t 0.059693498614713326 \t 0.10449491755689014\n",
      "--- 0.744 seconds in epoch ---\n",
      "\n",
      "Epoch 216\n",
      "mean training loss\t 0.13211516347087798\n",
      "epoch 216 '\n",
      "MSE for train and val\t 0.055658869436252756 \t 0.10077105165334224\n",
      "--- 0.771 seconds in epoch ---\n",
      "\n",
      "Epoch 217\n",
      "mean training loss\t 0.13186310075345586\n",
      "epoch 217 '\n",
      "MSE for train and val\t 0.054802626033065086 \t 0.10017963756297295\n",
      "--- 0.769 seconds in epoch ---\n",
      "\n",
      "Epoch 218\n",
      "mean training loss\t 0.13173867410323659\n",
      "epoch 218 '\n",
      "MSE for train and val\t 0.05573383720363813 \t 0.1004331866204229\n",
      "--- 0.794 seconds in epoch ---\n",
      "\n",
      "Epoch 219\n",
      "mean training loss\t 0.13152796801973562\n",
      "epoch 219 '\n",
      "MSE for train and val\t 0.056265208914213695 \t 0.10195972865318138\n",
      "--- 0.728 seconds in epoch ---\n",
      "\n",
      "Epoch 220\n",
      "mean training loss\t 0.13159399970633084\n",
      "epoch 220 '\n",
      "MSE for train and val\t 0.05496589299208512 \t 0.10003956213525673\n",
      "--- 0.803 seconds in epoch ---\n",
      "\n",
      "Epoch 221\n",
      "mean training loss\t 0.13156248890474195\n",
      "epoch 221 '\n",
      "MSE for train and val\t 0.057547952702080706 \t 0.10271782948989067\n",
      "--- 0.742 seconds in epoch ---\n",
      "\n",
      "Epoch 222\n",
      "mean training loss\t 0.131712570395626\n",
      "epoch 222 '\n",
      "MSE for train and val\t 0.05722581234392401 \t 0.10361795304035379\n",
      "--- 0.767 seconds in epoch ---\n",
      "\n",
      "Epoch 223\n",
      "mean training loss\t 0.13124679788702823\n",
      "epoch 223 '\n",
      "MSE for train and val\t 0.05511452906834721 \t 0.10065999766908823\n",
      "--- 0.787 seconds in epoch ---\n",
      "\n",
      "Epoch 224\n",
      "mean training loss\t 0.1313558632233104\n",
      "epoch 224 '\n",
      "MSE for train and val\t 0.056938355398637765 \t 0.10143680046309696\n",
      "--- 0.755 seconds in epoch ---\n",
      "\n",
      "Epoch 225\n",
      "mean training loss\t 0.13103919720552007\n",
      "epoch 225 '\n",
      "MSE for train and val\t 0.054758511453572335 \t 0.09975018069415006\n",
      "--- 0.777 seconds in epoch ---\n",
      "\n",
      "Epoch 226\n",
      "mean training loss\t 0.1310221916827999\n",
      "epoch 226 '\n",
      "MSE for train and val\t 0.05429366702047084 \t 0.09916557477126614\n",
      "--- 0.789 seconds in epoch ---\n",
      "\n",
      "Epoch 227\n",
      "mean training loss\t 0.1310486314238095\n",
      "epoch 227 '\n",
      "MSE for train and val\t 0.05461442205686211 \t 0.09983818454219959\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 228\n",
      "mean training loss\t 0.1311641512835612\n",
      "epoch 228 '\n",
      "MSE for train and val\t 0.057491079989365014 \t 0.1033309157966768\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 229\n",
      "mean training loss\t 0.13163083278741994\n",
      "epoch 229 '\n",
      "MSE for train and val\t 0.0725434372371233 \t 0.11608750635315443\n",
      "--- 0.753 seconds in epoch ---\n",
      "\n",
      "Epoch 230\n",
      "mean training loss\t 0.13073221850590627\n",
      "epoch 230 '\n",
      "MSE for train and val\t 0.054894805870136267 \t 0.0997133299699247\n",
      "--- 0.8 seconds in epoch ---\n",
      "\n",
      "Epoch 231\n",
      "mean training loss\t 0.13071989676014323\n",
      "epoch 231 '\n",
      "MSE for train and val\t 0.06313198573591312 \t 0.10980474548073166\n",
      "--- 0.773 seconds in epoch ---\n",
      "\n",
      "Epoch 232\n",
      "mean training loss\t 0.13282516857639687\n",
      "epoch 232 '\n",
      "MSE for train and val\t 0.06513988695377365 \t 0.10936010275396749\n",
      "--- 0.754 seconds in epoch ---\n",
      "\n",
      "Epoch 233\n",
      "mean training loss\t 0.13042565095620076\n",
      "epoch 233 '\n",
      "MSE for train and val\t 0.054890926747836896 \t 0.10081969524322114\n",
      "--- 0.762 seconds in epoch ---\n",
      "\n",
      "Epoch 234\n",
      "mean training loss\t 0.13019957063628024\n",
      "epoch 234 '\n",
      "MSE for train and val\t 0.055259971539796435 \t 0.09983042094331908\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 235\n",
      "mean training loss\t 0.13010011650011188\n",
      "epoch 235 '\n",
      "MSE for train and val\t 0.05346035036404842 \t 0.0988058225105383\n",
      "--- 0.763 seconds in epoch ---\n",
      "\n",
      "Epoch 236\n",
      "mean training loss\t 0.1300075926985897\n",
      "epoch 236 '\n",
      "MSE for train and val\t 0.053975320170998556 \t 0.09947456219277213\n",
      "--- 0.777 seconds in epoch ---\n",
      "\n",
      "Epoch 237\n",
      "mean training loss\t 0.12992770527718497\n",
      "epoch 237 '\n",
      "MSE for train and val\t 0.05371239807318424 \t 0.09909621928414372\n",
      "--- 0.791 seconds in epoch ---\n",
      "\n",
      "Epoch 238\n",
      "mean training loss\t 0.1297933749732424\n",
      "epoch 238 '\n",
      "MSE for train and val\t 0.05534566282357154 \t 0.1012160419450092\n",
      "--- 0.795 seconds in epoch ---\n",
      "\n",
      "Epoch 239\n",
      "mean training loss\t 0.12982966992698733\n",
      "epoch 239 '\n",
      "MSE for train and val\t 0.05395776765266931 \t 0.09979819261522342\n",
      "--- 0.758 seconds in epoch ---\n",
      "\n",
      "Epoch 240\n",
      "mean training loss\t 0.12976632787555945\n",
      "epoch 240 '\n",
      "MSE for train and val\t 0.05362896363337054 \t 0.09912772100618308\n",
      "--- 0.802 seconds in epoch ---\n",
      "\n",
      "Epoch 241\n",
      "mean training loss\t 0.12961861398376404\n",
      "epoch 241 '\n",
      "MSE for train and val\t 0.05395161560340797 \t 0.09966197353926673\n",
      "--- 0.771 seconds in epoch ---\n",
      "\n",
      "Epoch 242\n",
      "mean training loss\t 0.12948900634636645\n",
      "epoch 242 '\n",
      "MSE for train and val\t 0.053516892120505054 \t 0.09874537872641191\n",
      "--- 0.81 seconds in epoch ---\n",
      "\n",
      "Epoch 243\n",
      "mean training loss\t 0.1296447578267973\n",
      "epoch 243 '\n",
      "MSE for train and val\t 0.05556561442645802 \t 0.101040464864121\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 244\n",
      "mean training loss\t 0.129277119187058\n",
      "epoch 244 '\n",
      "MSE for train and val\t 0.054295507556658504 \t 0.10120912143853726\n",
      "--- 0.748 seconds in epoch ---\n",
      "\n",
      "Epoch 245\n",
      "mean training loss\t 0.12930722231747674\n",
      "epoch 245 '\n",
      "MSE for train and val\t 0.05369879776527655 \t 0.0992824997379114\n",
      "--- 0.742 seconds in epoch ---\n",
      "\n",
      "Epoch 246\n",
      "mean training loss\t 0.12918148326580642\n",
      "epoch 246 '\n",
      "MSE for train and val\t 0.054040238958702 \t 0.09949835539700964\n",
      "--- 0.823 seconds in epoch ---\n",
      "\n",
      "Epoch 247\n",
      "mean training loss\t 0.12939371256554713\n",
      "epoch 247 '\n",
      "MSE for train and val\t 0.05373735063866428 \t 0.10041815466590777\n",
      "--- 0.748 seconds in epoch ---\n",
      "\n",
      "Epoch 248\n",
      "mean training loss\t 0.1291454381630069\n",
      "epoch 248 '\n",
      "MSE for train and val\t 0.053879649065157756 \t 0.09986922531671479\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 249\n",
      "mean training loss\t 0.12909010543686444\n",
      "epoch 249 '\n",
      "MSE for train and val\t 0.05451617897689235 \t 0.10004525693859113\n",
      "--- 0.776 seconds in epoch ---\n",
      "\n",
      "Epoch 250\n",
      "mean training loss\t 0.1289670527714198\n",
      "epoch 250 '\n",
      "MSE for train and val\t 0.052985081226217985 \t 0.09921515169609184\n",
      "--- 0.78 seconds in epoch ---\n",
      "\n",
      "Epoch 251\n",
      "mean training loss\t 0.12871870449820502\n",
      "epoch 251 '\n",
      "MSE for train and val\t 0.05333258064824943 \t 0.09972380275013315\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 252\n",
      "mean training loss\t 0.12869951939973676\n",
      "epoch 252 '\n",
      "MSE for train and val\t 0.05300934748499912 \t 0.09886922544026779\n",
      "--- 0.834 seconds in epoch ---\n",
      "\n",
      "Epoch 253\n",
      "mean training loss\t 0.1285219064257184\n",
      "epoch 253 '\n",
      "MSE for train and val\t 0.05308151392062484 \t 0.09908594969054163\n",
      "--- 0.817 seconds in epoch ---\n",
      "\n",
      "Epoch 254\n",
      "mean training loss\t 0.12833594354938288\n",
      "epoch 254 '\n",
      "MSE for train and val\t 0.05369401532432108 \t 0.09978930766853852\n",
      "--- 0.845 seconds in epoch ---\n",
      "\n",
      "Epoch 255\n",
      "mean training loss\t 0.12834285903172415\n",
      "epoch 255 '\n",
      "MSE for train and val\t 0.05270686881725632 \t 0.09857497718417293\n",
      "--- 0.795 seconds in epoch ---\n",
      "\n",
      "Epoch 256\n",
      "mean training loss\t 0.1283138387027334\n",
      "epoch 256 '\n",
      "MSE for train and val\t 0.05255177469541576 \t 0.09872824887907765\n",
      "--- 0.802 seconds in epoch ---\n",
      "\n",
      "Epoch 257\n",
      "mean training loss\t 0.1282506612480664\n",
      "epoch 257 '\n",
      "MSE for train and val\t 0.053530143961558904 \t 0.09952227509899207\n",
      "--- 0.805 seconds in epoch ---\n",
      "\n",
      "Epoch 258\n",
      "mean training loss\t 0.12818570127252674\n",
      "epoch 258 '\n",
      "MSE for train and val\t 0.05348600800103243 \t 0.09936337198532491\n",
      "--- 0.78 seconds in epoch ---\n",
      "\n",
      "Epoch 259\n",
      "mean training loss\t 0.12805451132723542\n",
      "epoch 259 '\n",
      "MSE for train and val\t 0.054580676340994234 \t 0.10084793302465672\n",
      "--- 0.755 seconds in epoch ---\n",
      "\n",
      "Epoch 260\n",
      "mean training loss\t 0.12803716818328764\n",
      "epoch 260 '\n",
      "MSE for train and val\t 0.05282725359538976 \t 0.09884452210323921\n",
      "--- 0.788 seconds in epoch ---\n",
      "\n",
      "Epoch 261\n",
      "mean training loss\t 0.1279779389500618\n",
      "epoch 261 '\n",
      "MSE for train and val\t 0.05322857031120366 \t 0.09963270034210128\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.127575284975474\n",
      "epoch 262 '\n",
      "MSE for train and val\t 0.05297095990655569 \t 0.09942729451960666\n",
      "--- 0.77 seconds in epoch ---\n",
      "\n",
      "Epoch 263\n",
      "mean training loss\t 0.12791040849978808\n",
      "epoch 263 '\n",
      "MSE for train and val\t 0.05739934483348779 \t 0.10311229321115563\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 264\n",
      "mean training loss\t 0.12775409844566565\n",
      "epoch 264 '\n",
      "MSE for train and val\t 0.053367373141423605 \t 0.09931782567448343\n",
      "--- 0.767 seconds in epoch ---\n",
      "\n",
      "Epoch 265\n",
      "mean training loss\t 0.12779904860453528\n",
      "epoch 265 '\n",
      "MSE for train and val\t 0.05259719598289639 \t 0.09846403955168775\n",
      "--- 0.793 seconds in epoch ---\n",
      "\n",
      "Epoch 266\n",
      "mean training loss\t 0.1274500885947806\n",
      "epoch 266 '\n",
      "MSE for train and val\t 0.05268544523211108 \t 0.09877760422846696\n",
      "--- 0.771 seconds in epoch ---\n",
      "\n",
      "Epoch 267\n",
      "mean training loss\t 0.12717183254292755\n",
      "epoch 267 '\n",
      "MSE for train and val\t 0.05215953608699484 \t 0.09854021496765777\n",
      "--- 0.82 seconds in epoch ---\n",
      "\n",
      "Epoch 268\n",
      "mean training loss\t 0.127455557736217\n",
      "epoch 268 '\n",
      "MSE for train and val\t 0.0542068629121805 \t 0.09999415667331565\n",
      "--- 0.753 seconds in epoch ---\n",
      "\n",
      "Epoch 269\n",
      "mean training loss\t 0.12733403710556812\n",
      "epoch 269 '\n",
      "MSE for train and val\t 0.05269266543286763 \t 0.09951032069405114\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 270\n",
      "mean training loss\t 0.12709481647757234\n",
      "epoch 270 '\n",
      "MSE for train and val\t 0.05209703654682256 \t 0.09880683964511518\n",
      "--- 0.784 seconds in epoch ---\n",
      "\n",
      "Epoch 271\n",
      "mean training loss\t 0.12700997613003998\n",
      "epoch 271 '\n",
      "MSE for train and val\t 0.05238295554187485 \t 0.0986514349129504\n",
      "--- 0.75 seconds in epoch ---\n",
      "\n",
      "Epoch 272\n",
      "mean training loss\t 0.126991441137478\n",
      "epoch 272 '\n",
      "MSE for train and val\t 0.05202138010626486 \t 0.09883672129142808\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 273\n",
      "mean training loss\t 0.12692355144707884\n",
      "epoch 273 '\n",
      "MSE for train and val\t 0.05280698367729241 \t 0.1006741368107497\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 274\n",
      "mean training loss\t 0.12678364930582828\n",
      "epoch 274 '\n",
      "MSE for train and val\t 0.0523185799069627 \t 0.0989352223248965\n",
      "--- 0.761 seconds in epoch ---\n",
      "\n",
      "Epoch 275\n",
      "mean training loss\t 0.12687783773805275\n",
      "epoch 275 '\n",
      "MSE for train and val\t 0.052894640867537764 \t 0.09976283743321226\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 276\n",
      "mean training loss\t 0.12680922771086459\n",
      "epoch 276 '\n",
      "MSE for train and val\t 0.05372642115990606 \t 0.10024278311034714\n",
      "--- 0.797 seconds in epoch ---\n",
      "\n",
      "Epoch 277\n",
      "mean training loss\t 0.12663366821457128\n",
      "epoch 277 '\n",
      "MSE for train and val\t 0.0515084318109442 \t 0.09848857427724925\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 278\n",
      "mean training loss\t 0.12647142803571262\n",
      "epoch 278 '\n",
      "MSE for train and val\t 0.05207595381410884 \t 0.09839195436592338\n",
      "--- 0.774 seconds in epoch ---\n",
      "\n",
      "Epoch 279\n",
      "mean training loss\t 0.1262883673925869\n",
      "epoch 279 '\n",
      "MSE for train and val\t 0.051978177995560475 \t 0.09845469728992004\n",
      "--- 0.822 seconds in epoch ---\n",
      "\n",
      "Epoch 280\n",
      "mean training loss\t 0.12628924748936637\n",
      "epoch 280 '\n",
      "MSE for train and val\t 0.05552286884039519 \t 0.10375371861311537\n",
      "--- 0.763 seconds in epoch ---\n",
      "\n",
      "Epoch 281\n",
      "mean training loss\t 0.12624657703716247\n",
      "epoch 281 '\n",
      "MSE for train and val\t 0.05166552547036887 \t 0.09829515247250019\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 282\n",
      "mean training loss\t 0.1271079035811737\n",
      "epoch 282 '\n",
      "MSE for train and val\t 0.07131515246519171 \t 0.1168012906743185\n",
      "--- 0.803 seconds in epoch ---\n",
      "\n",
      "Epoch 283\n",
      "mean training loss\t 0.12665153976346627\n",
      "epoch 283 '\n",
      "MSE for train and val\t 0.05311545677533113 \t 0.09915998180660451\n",
      "--- 0.776 seconds in epoch ---\n",
      "\n",
      "Epoch 284\n",
      "mean training loss\t 0.12639612833984562\n",
      "epoch 284 '\n",
      "MSE for train and val\t 0.05933380376125602 \t 0.10533367063975015\n",
      "--- 0.766 seconds in epoch ---\n",
      "\n",
      "Epoch 285\n",
      "mean training loss\t 0.12602890788531695\n",
      "epoch 285 '\n",
      "MSE for train and val\t 0.05152966510871906 \t 0.09802976817145119\n",
      "--- 0.783 seconds in epoch ---\n",
      "\n",
      "Epoch 286\n",
      "mean training loss\t 0.12577881803278063\n",
      "epoch 286 '\n",
      "MSE for train and val\t 0.05107755781423864 \t 0.09779372260171228\n",
      "--- 0.803 seconds in epoch ---\n",
      "\n",
      "Epoch 287\n",
      "mean training loss\t 0.12591086604067536\n",
      "epoch 287 '\n",
      "MSE for train and val\t 0.051633064589286945 \t 0.09874636057226562\n",
      "--- 0.813 seconds in epoch ---\n",
      "\n",
      "Epoch 288\n",
      "mean training loss\t 0.12581023830370824\n",
      "epoch 288 '\n",
      "MSE for train and val\t 0.0526905758044614 \t 0.10002781912665584\n",
      "--- 0.778 seconds in epoch ---\n",
      "\n",
      "Epoch 289\n",
      "mean training loss\t 0.12580721507795523\n",
      "epoch 289 '\n",
      "MSE for train and val\t 0.05436560160874906 \t 0.10131350157826385\n",
      "--- 0.795 seconds in epoch ---\n",
      "\n",
      "Epoch 290\n",
      "mean training loss\t 0.1256291745871794\n",
      "epoch 290 '\n",
      "MSE for train and val\t 0.05456059667425346 \t 0.10098507974281815\n",
      "--- 0.723 seconds in epoch ---\n",
      "\n",
      "Epoch 291\n",
      "mean training loss\t 0.12587929898109593\n",
      "epoch 291 '\n",
      "MSE for train and val\t 0.05481773808483412 \t 0.10087393138611209\n",
      "--- 0.802 seconds in epoch ---\n",
      "\n",
      "Epoch 292\n",
      "mean training loss\t 0.1255334539980185\n",
      "epoch 292 '\n",
      "MSE for train and val\t 0.05147451476318068 \t 0.09858628799306622\n",
      "--- 0.764 seconds in epoch ---\n",
      "\n",
      "Epoch 293\n",
      "mean training loss\t 0.12538894019165978\n",
      "epoch 293 '\n",
      "MSE for train and val\t 0.051976565049532825 \t 0.09903888589760174\n",
      "--- 0.801 seconds in epoch ---\n",
      "\n",
      "Epoch 294\n",
      "mean training loss\t 0.12546341531589383\n",
      "epoch 294 '\n",
      "MSE for train and val\t 0.05345347182279973 \t 0.09962118038648726\n",
      "--- 0.798 seconds in epoch ---\n",
      "\n",
      "Epoch 295\n",
      "mean training loss\t 0.12517045172022992\n",
      "epoch 295 '\n",
      "MSE for train and val\t 0.05078064169621437 \t 0.09842767517529803\n",
      "--- 0.772 seconds in epoch ---\n",
      "\n",
      "Epoch 296\n",
      "mean training loss\t 0.12532733769201843\n",
      "epoch 296 '\n",
      "MSE for train and val\t 0.05324571793864085 \t 0.10056436675555343\n",
      "--- 0.821 seconds in epoch ---\n",
      "\n",
      "Epoch 297\n",
      "mean training loss\t 0.12505490660178856\n",
      "epoch 297 '\n",
      "MSE for train and val\t 0.051230641052036674 \t 0.09873994200708253\n",
      "--- 0.77 seconds in epoch ---\n",
      "\n",
      "Epoch 298\n",
      "mean training loss\t 0.12511165559780402\n",
      "epoch 298 '\n",
      "MSE for train and val\t 0.05093775021438764 \t 0.09849636203915163\n",
      "--- 0.795 seconds in epoch ---\n",
      "\n",
      "Epoch 299\n",
      "mean training loss\t 0.12480181345685584\n",
      "epoch 299 '\n",
      "MSE for train and val\t 0.05127568768468546 \t 0.09899681605588714\n",
      "--- 0.791 seconds in epoch ---\n",
      "\n",
      "FULLY TRAINED USING 233.71884679794312 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from data_loader import loadDataset\n",
    "\n",
    "\n",
    "#batch size, epochs, learning rate\n",
    "BS = 32\n",
    "EP = 300\n",
    "LR = 5e-3\n",
    "\n",
    "data_path = 'data/'\n",
    "trnX1,trnY1,    tstX,tstY = loadDataset(data_path)\n",
    "\n",
    "#whitening the input and output data\n",
    "trnX1 = trnX1 - np.expand_dims(np.mean(trnX1,axis=0),axis=0)\n",
    "trnX1 = trnX1 / np.expand_dims(np.sqrt(np.mean(trnX1**2,axis=0)),axis=0)\n",
    "trnX1_og = trnX1    \n",
    "trnY1_og = trnY1[:,None]-4.42\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "if True:\n",
    "    mn=0\n",
    "    M_NUM = trnX1_og.shape[0]\n",
    "    rand_indices = np.random.permutation(M_NUM)\n",
    "    per = .7\n",
    "    M_TRN_NUM = int(M_NUM*per)\n",
    "    trnX1 = trnX1_og[rand_indices[:M_TRN_NUM]]\n",
    "    trnY1 = trnY1_og[rand_indices[:M_TRN_NUM]]\n",
    "    valX1 = trnX1_og[rand_indices[M_TRN_NUM:]]\n",
    "    valY1 = trnY1_og[rand_indices[M_TRN_NUM:]]\n",
    "\n",
    "\n",
    "    trn_data1 = TensorDataset(torch.from_numpy(trnX1).float().to(device),\n",
    "                              torch.from_numpy(trnY1).float().to(device))\n",
    "    trn_loader1 =DataLoader(dataset=trn_data1, batch_size=BS,shuffle=True)\n",
    "\n",
    "    trn_tensor = torch.from_numpy(trnX1).float().to(device)\n",
    "    val_tensor = torch.from_numpy(valX1).float().to(device)\n",
    "\n",
    "\n",
    "    sizes = [13,256,128,64,1]\n",
    "    net1 = MLP(sizes).to(device)\n",
    "    opt1 = optim.Adagrad(net1.parameters(), lr = LR)\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_trn_accs = np.zeros(EP)\n",
    "    all_val_accs = np.zeros(EP)\n",
    "    all_losses = np.zeros((EP,len(trn_loader1),7))\n",
    "    all_trn_losses = np.zeros((EP,len(trn_loader1)))\n",
    "    all_val_losses = np.zeros(EP)\n",
    "\n",
    "    best_val_score = -100\n",
    "    best_net = None\n",
    "\n",
    "    full_training_start_time = time.time()\n",
    "    for k in range(EP):\n",
    "        print('Epoch',k)\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        #---TRAINING PHASE---\n",
    "        for j,(x_batch,y_batch) in enumerate(trn_loader1):\n",
    "            net1.train()\n",
    "            logits = net1(x_batch)\n",
    "            dnn_logits,gam_logits,shape_loss = net1(x_batch)\n",
    "            logits = dnn_logits + gam_logits\n",
    "            \n",
    "\n",
    "            l1_reg = torch.zeros(1).to(device)\n",
    "            all_linear_params = net1.collectParameters()\n",
    "            lambda1 = 5e-5 \n",
    "            l1_reg = lambda1 * torch.norm(all_linear_params, 1)\n",
    "\n",
    "\n",
    "            mseloss = (y_batch.narrow(1,0,1) - logits.narrow(1,0,1))**2\n",
    "            mseloss = torch.mean(mseloss)\n",
    "            mseloss = mseloss\n",
    "            \n",
    "            \n",
    "            all_trn_losses[k,j] = mseloss.item()\n",
    "            loss = mseloss + l1_reg\n",
    "            loss.backward()\n",
    "            all_losses[k,j,0] = loss.item()\n",
    "            all_losses[k,j,1] = mseloss.item()\n",
    "            all_losses[k,j,2] = l1_reg.item()\n",
    "            \n",
    "            \n",
    "            opt1.step()\n",
    "            opt1.zero_grad()\n",
    "        print('mean training loss\\t',np.mean(all_losses[k,:,0]))\n",
    "        \n",
    "        \n",
    "        #---VALIDATION PHASE---\n",
    "        print('epoch',k,'\\'')\n",
    "        net1.eval()\n",
    "        #training accuracy\n",
    "        logits = net1(trn_tensor)\n",
    "        logits = logits[0]+logits[1]\n",
    "        logits = logits.cpu().detach().numpy()[:,0]\n",
    "        all_trn_accs[k] = np.mean(((trnY1[:,0])-logits)**2)\n",
    "        #validation accuracy\n",
    "        logits = net1(val_tensor)\n",
    "        logits = logits[0]+logits[1]\n",
    "        logits = logits.cpu().detach().numpy()[:,0]\n",
    "        all_val_accs[k] = np.mean(((valY1[:,0])-logits)**2)\n",
    "        all_val_losses[k] = np.mean(((valY1[:,0])-logits)**2)\n",
    "        print('MSE for train and val\\t',all_trn_accs[k],'\\t',all_val_accs[k])\n",
    "\n",
    "\n",
    "        \n",
    "        if k%1==0:\n",
    "            val_score = -all_val_accs[k]\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                best_net = copy.deepcopy(net1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"--- %s seconds in epoch ---\" % round((end_time - start_time),3))\n",
    "        print()\n",
    "    print('FULLY TRAINED USING',time.time()-full_training_start_time,'seconds')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.mean(all_losses[:k,:,0],axis=1))\n",
    "plt.plot(all_val_losses[:k])\n",
    "plt.plot(np.mean(all_losses[:k,:,1],axis=1))\n",
    "plt.plot(np.mean(all_losses[:k,:,2],axis=1))\n",
    "plt.plot([-50,EP+50],[0,0],c='k')\n",
    "plt.legend(['trn','val','mse','l1'])\n",
    "plt.xlim(-20,EP+20)\n",
    "plt.ylim(-.01,.45)\n",
    "plt.title(\"DNN/MLP Loss While Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_wrappers import MixedModelWrapperTorch\n",
    "device=torch.device(\"cpu\")\n",
    "\n",
    "model_wrap_MLP = MixedModelWrapperTorch(net1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregate_detections import aggregateContrastiveDetections_only1D, aggregateContrastiveDetections_only2D\n",
    "from notebook_utils import prettyPrintInteractionSingles,prettyPrintInteractionPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding and plotting the one-dimensional interaction/ main effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0\n",
      "\t 10\n",
      "\t 20\n",
      "\t 30\n",
      "\t 40\n",
      "\t 50\n",
      "\t 60\n",
      "\t 70\n",
      "\t 80\n",
      "\t 90\n"
     ]
    }
   ],
   "source": [
    "AGG_K=100\n",
    "agg_1,mains_1,grads_1,_ = aggregateContrastiveDetections_only1D(model_wrap_MLP,valX1,AGG_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-5,2,11)\n",
    "plt.hist(np.log(list(agg_1.values())),color='r',alpha=.34,bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Accumulated 1D Interactions\n",
      "-------------------------------\n",
      "4\n",
      "hour           ;\n",
      "\t\t 2.5156340482447965\n",
      "\n",
      "7\n",
      "workday        ;\n",
      "\t\t 0.887338962339528\n",
      "\n",
      "2\n",
      "year           ;\n",
      "\t\t 0.3440553390375723\n",
      "\n",
      "5\n",
      "holiday        ;\n",
      "\t\t 0.3191499120176296\n",
      "\n",
      "9\n",
      "temperature    ;\n",
      "\t\t 0.156226054882083\n",
      "\n",
      "1\n",
      "season         ;\n",
      "\t\t 0.08843247676323514\n",
      "\n",
      "3\n",
      "month          ;\n",
      "\t\t 0.0873274609826552\n",
      "\n",
      "8\n",
      "weather        ;\n",
      "\t\t 0.08622823881557257\n",
      "\n",
      "10\n",
      "feels_like_temp;\n",
      "\t\t 0.07178829825908106\n",
      "\n",
      "11\n",
      "humidity       ;\n",
      "\t\t 0.05293079293844226\n",
      "\n",
      "6\n",
      "day of week    ;\n",
      "\t\t 0.05181468724190819\n",
      "\n",
      "0\n",
      "day            ;\n",
      "\t\t 0.026508921983901892\n",
      "\n",
      "12\n",
      "wind speed     ;\n",
      "\t\t 0.013186776572698983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prettyPrintInteractionSingles(agg_1,readable_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding and plotting the two-dimensional interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0\n",
      "\t 10\n",
      "\t 20\n",
      "\t 30\n",
      "\t 40\n",
      "\t 50\n",
      "\t 60\n",
      "\t 70\n",
      "\t 80\n",
      "\t 90\n"
     ]
    }
   ],
   "source": [
    "AGG_K=100\n",
    "agg_2,mains_2,grads_2,_ = aggregateContrastiveDetections_only2D(model_wrap_MLP,valX1,AGG_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-5,2,11)\n",
    "plt.hist(np.log(list(agg_2.values())),color='b',alpha=.34)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Accumulated 2D Interactions\n",
      "-------------------------------\n",
      "4 7\n",
      "hour           ; workday        \n",
      "\t\t 1.945275395337919\n",
      "\n",
      "5 7\n",
      "holiday        ; workday        \n",
      "\t\t 0.5558336827641437\n",
      "\n",
      "4 5\n",
      "hour           ; holiday        \n",
      "\t\t 0.5423296005838537\n",
      "\n",
      "5 10\n",
      "holiday        ; feels_like_temp\n",
      "\t\t 0.24234767812449087\n",
      "\n",
      "4 9\n",
      "hour           ; temperature    \n",
      "\t\t 0.17469543548392852\n",
      "\n",
      "1 5\n",
      "season         ; holiday        \n",
      "\t\t 0.15737598639769057\n",
      "\n",
      "5 9\n",
      "holiday        ; temperature    \n",
      "\t\t 0.15314240233984902\n",
      "\n",
      "2 5\n",
      "year           ; holiday        \n",
      "\t\t 0.13754884400370845\n",
      "\n",
      "1 4\n",
      "season         ; hour           \n",
      "\t\t 0.13543182744155494\n",
      "\n",
      "1 3\n",
      "season         ; month          \n",
      "\t\t 0.12814374220675395\n",
      "\n",
      "1 7\n",
      "season         ; workday        \n",
      "\t\t 0.12351030913833894\n",
      "\n",
      "0 5\n",
      "day            ; holiday        \n",
      "\t\t 0.11142789944647997\n",
      "\n",
      "2 7\n",
      "year           ; workday        \n",
      "\t\t 0.10767525034551763\n",
      "\n",
      "6 7\n",
      "day of week    ; workday        \n",
      "\t\t 0.10290141269784533\n",
      "\n",
      "4 10\n",
      "hour           ; feels_like_temp\n",
      "\t\t 0.10264169289310417\n",
      "\n",
      "3 7\n",
      "month          ; workday        \n",
      "\t\t 0.09544764014778469\n",
      "\n",
      "7 9\n",
      "workday        ; temperature    \n",
      "\t\t 0.09434232572848632\n",
      "\n",
      "9 10\n",
      "temperature    ; feels_like_temp\n",
      "\t\t 0.09216202141185613\n",
      "\n",
      "5 8\n",
      "holiday        ; weather        \n",
      "\t\t 0.0882732924598437\n",
      "\n",
      "2 4\n",
      "year           ; hour           \n",
      "\t\t 0.08456597262112354\n",
      "\n",
      "4 6\n",
      "hour           ; day of week    \n",
      "\t\t 0.08386143846351679\n",
      "\n",
      "3 5\n",
      "month          ; holiday        \n",
      "\t\t 0.07561020352088331\n",
      "\n",
      "7 8\n",
      "workday        ; weather        \n",
      "\t\t 0.07441958194355776\n",
      "\n",
      "3 4\n",
      "month          ; hour           \n",
      "\t\t 0.06974551904130664\n",
      "\n",
      "2 9\n",
      "year           ; temperature    \n",
      "\t\t 0.06402659101010098\n",
      "\n",
      "4 8\n",
      "hour           ; weather        \n",
      "\t\t 0.06054453569658131\n",
      "\n",
      "1 2\n",
      "season         ; year           \n",
      "\t\t 0.05941642783068666\n",
      "\n",
      "2 8\n",
      "year           ; weather        \n",
      "\t\t 0.05702021597768027\n",
      "\n",
      "1 8\n",
      "season         ; weather        \n",
      "\t\t 0.056066819529368574\n",
      "\n",
      "1 10\n",
      "season         ; feels_like_temp\n",
      "\t\t 0.055511740972133024\n",
      "\n",
      "5 6\n",
      "holiday        ; day of week    \n",
      "\t\t 0.05528811632116353\n",
      "\n",
      "7 10\n",
      "workday        ; feels_like_temp\n",
      "\t\t 0.05501453980052868\n",
      "\n",
      "4 11\n",
      "hour           ; humidity       \n",
      "\t\t 0.05229779577812118\n",
      "\n",
      "3 10\n",
      "month          ; feels_like_temp\n",
      "\t\t 0.049126245901823955\n",
      "\n",
      "1 9\n",
      "season         ; temperature    \n",
      "\t\t 0.04761337732334009\n",
      "\n",
      "0 3\n",
      "day            ; month          \n",
      "\t\t 0.04688443323887435\n",
      "\n",
      "5 11\n",
      "holiday        ; humidity       \n",
      "\t\t 0.046534840750187004\n",
      "\n",
      "5 12\n",
      "holiday        ; wind speed     \n",
      "\t\t 0.039055887134220466\n",
      "\n",
      "2 3\n",
      "year           ; month          \n",
      "\t\t 0.034920266012116855\n",
      "\n",
      "8 9\n",
      "weather        ; temperature    \n",
      "\t\t 0.03483575113187108\n",
      "\n",
      "2 6\n",
      "year           ; day of week    \n",
      "\t\t 0.034555011918612495\n",
      "\n",
      "8 11\n",
      "weather        ; humidity       \n",
      "\t\t 0.033921756496314036\n",
      "\n",
      "0 7\n",
      "day            ; workday        \n",
      "\t\t 0.03379426003341665\n",
      "\n",
      "0 1\n",
      "day            ; season         \n",
      "\t\t 0.03224067179892807\n",
      "\n",
      "8 10\n",
      "weather        ; feels_like_temp\n",
      "\t\t 0.03220028810171019\n",
      "\n",
      "2 10\n",
      "year           ; feels_like_temp\n",
      "\t\t 0.031314776584732126\n",
      "\n",
      "7 12\n",
      "workday        ; wind speed     \n",
      "\t\t 0.031106778415406933\n",
      "\n",
      "3 8\n",
      "month          ; weather        \n",
      "\t\t 0.03044421847555088\n",
      "\n",
      "0 4\n",
      "day            ; hour           \n",
      "\t\t 0.030232611829284748\n",
      "\n",
      "1 6\n",
      "season         ; day of week    \n",
      "\t\t 0.028412039351435273\n",
      "\n",
      "7 11\n",
      "workday        ; humidity       \n",
      "\t\t 0.028302125943196622\n",
      "\n",
      "3 9\n",
      "month          ; temperature    \n",
      "\t\t 0.02811360347968236\n",
      "\n",
      "2 12\n",
      "year           ; wind speed     \n",
      "\t\t 0.026961109714790123\n",
      "\n",
      "0 2\n",
      "day            ; year           \n",
      "\t\t 0.026469527748576145\n",
      "\n",
      "6 8\n",
      "day of week    ; weather        \n",
      "\t\t 0.02559442100597361\n",
      "\n",
      "3 6\n",
      "month          ; day of week    \n",
      "\t\t 0.025316518643758147\n",
      "\n",
      "1 11\n",
      "season         ; humidity       \n",
      "\t\t 0.024960594348898486\n",
      "\n",
      "4 12\n",
      "hour           ; wind speed     \n",
      "\t\t 0.024131855747774315\n",
      "\n",
      "6 9\n",
      "day of week    ; temperature    \n",
      "\t\t 0.023810195371141124\n",
      "\n",
      "10 11\n",
      "feels_like_temp; humidity       \n",
      "\t\t 0.023580759006295623\n",
      "\n",
      "8 12\n",
      "weather        ; wind speed     \n",
      "\t\t 0.022880502877526843\n",
      "\n",
      "2 11\n",
      "year           ; humidity       \n",
      "\t\t 0.022203751252591653\n",
      "\n",
      "9 11\n",
      "temperature    ; humidity       \n",
      "\t\t 0.018091719596654558\n",
      "\n",
      "0 8\n",
      "day            ; weather        \n",
      "\t\t 0.017582152138453055\n",
      "\n",
      "3 11\n",
      "month          ; humidity       \n",
      "\t\t 0.015927917343018395\n",
      "\n",
      "0 10\n",
      "day            ; feels_like_temp\n",
      "\t\t 0.015649391096520856\n",
      "\n",
      "0 6\n",
      "day            ; day of week    \n",
      "\t\t 0.014752279749345858\n",
      "\n",
      "3 12\n",
      "month          ; wind speed     \n",
      "\t\t 0.01463832578125479\n",
      "\n",
      "1 12\n",
      "season         ; wind speed     \n",
      "\t\t 0.014557165130902617\n",
      "\n",
      "9 12\n",
      "temperature    ; wind speed     \n",
      "\t\t 0.014478030066552674\n",
      "\n",
      "6 11\n",
      "day of week    ; humidity       \n",
      "\t\t 0.014304632873242455\n",
      "\n",
      "0 11\n",
      "day            ; humidity       \n",
      "\t\t 0.01331120476077482\n",
      "\n",
      "6 10\n",
      "day of week    ; feels_like_temp\n",
      "\t\t 0.01302404502934781\n",
      "\n",
      "11 12\n",
      "humidity       ; wind speed     \n",
      "\t\t 0.010981473534060352\n",
      "\n",
      "0 9\n",
      "day            ; temperature    \n",
      "\t\t 0.010545219121361697\n",
      "\n",
      "6 12\n",
      "day of week    ; wind speed     \n",
      "\t\t 0.0104287836662031\n",
      "\n",
      "10 12\n",
      "feels_like_temp; wind speed     \n",
      "\t\t 0.009320653114214969\n",
      "\n",
      "0 12\n",
      "day            ; wind speed     \n",
      "\t\t 0.0073375635798349215\n",
      "\n",
      "indices2 49\n",
      "[(4, 7), (5, 7), (4, 5), (5, 10), (4, 9), (1, 5), (5, 9), (2, 5), (1, 4), (1, 3), (1, 7), (0, 5), (2, 7), (6, 7), (4, 10), (3, 7), (7, 9), (9, 10), (5, 8), (2, 4), (4, 6), (3, 5), (7, 8), (3, 4), (2, 9), (4, 8), (1, 2), (2, 8), (1, 8), (1, 10), (5, 6), (7, 10), (4, 11), (3, 10), (1, 9), (0, 3), (5, 11), (5, 12), (2, 3), (8, 9), (2, 6), (8, 11), (0, 7), (0, 1), (8, 10), (2, 10), (7, 12), (3, 8), (0, 4)]\n"
     ]
    }
   ],
   "source": [
    "prettyPrintInteractionPairs(agg_2,readable_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a SIAN-2 Generalized Additive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feel free to exchange these indices for a different set\n",
    "and explore the effects of changing the available interactions\n",
    "\n",
    "can compare to the families of indices used in Figure 2 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices1 = [(i,) for i in range(13)]\n",
    "indices2 = [(4, 7), (5, 7), (4, 5), (2, 5), (4, 10), (5, 8), (5, 10), (4, 9), (6, 7), (1, 3), (5, 9), (2, 4), (1, 7), (7, 10), (2, 7), (1, 4), (7, 9), (4, 6), (3, 7), (1, 5), (3, 4), (1, 2), (5, 6), (4, 8), (4, 11), (9, 10), (5, 11), (7, 8), (3, 5), (0, 5), (2, 10), (2, 8), (8, 11), (2, 9)]\n",
    "\n",
    "indices = []\n",
    "indices.extend(indices1)\n",
    "indices.extend(indices2)\n",
    "\n",
    "D = 13\n",
    "interactions_matrix = np.zeros((D,D),dtype=int)\n",
    "for (i,j) in indices2:\n",
    "    interactions_matrix[i,j] = 1\n",
    "plt.imshow(interactions_matrix) #plot of the interactions we are using\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "mean training loss\t 0.9097197927412439\n",
      "epoch 0 '\n",
      "MSE for train and val\t 0.6333648132508798 \t 0.6609520101870089\n",
      "--- 1.104 seconds in epoch ---\n",
      "\n",
      "Epoch 1\n",
      "mean training loss\t 0.6626342992313573\n",
      "epoch 1 '\n",
      "MSE for train and val\t 0.563695331319396 \t 0.5889457804980325\n",
      "--- 0.946 seconds in epoch ---\n",
      "\n",
      "Epoch 2\n",
      "mean training loss\t 0.606735283722643\n",
      "epoch 2 '\n",
      "MSE for train and val\t 0.5224143508674159 \t 0.5455284062091751\n",
      "--- 0.954 seconds in epoch ---\n",
      "\n",
      "Epoch 3\n",
      "mean training loss\t 0.5694085488553907\n",
      "epoch 3 '\n",
      "MSE for train and val\t 0.4833471425109601 \t 0.505215202284749\n",
      "--- 0.947 seconds in epoch ---\n",
      "\n",
      "Epoch 4\n",
      "mean training loss\t 0.5364052508698135\n",
      "epoch 4 '\n",
      "MSE for train and val\t 0.4560580922385569 \t 0.4766894911388825\n",
      "--- 0.95 seconds in epoch ---\n",
      "\n",
      "Epoch 5\n",
      "mean training loss\t 0.5026875336639217\n",
      "epoch 5 '\n",
      "MSE for train and val\t 0.4181899270139506 \t 0.4381737129330505\n",
      "--- 0.949 seconds in epoch ---\n",
      "\n",
      "Epoch 6\n",
      "mean training loss\t 0.47223335651100656\n",
      "epoch 6 '\n",
      "MSE for train and val\t 0.40232273928307694 \t 0.42123663374141634\n",
      "--- 0.957 seconds in epoch ---\n",
      "\n",
      "Epoch 7\n",
      "mean training loss\t 0.44150160018537865\n",
      "epoch 7 '\n",
      "MSE for train and val\t 0.3595930684537198 \t 0.3782035305038151\n",
      "--- 0.953 seconds in epoch ---\n",
      "\n",
      "Epoch 8\n",
      "mean training loss\t 0.4125744865077441\n",
      "epoch 8 '\n",
      "MSE for train and val\t 0.3327887573420377 \t 0.35035985864204133\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 9\n",
      "mean training loss\t 0.38687913339646135\n",
      "epoch 9 '\n",
      "MSE for train and val\t 0.30668675337257667 \t 0.32368558065110864\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 10\n",
      "mean training loss\t 0.36253335749516724\n",
      "epoch 10 '\n",
      "MSE for train and val\t 0.28485142308798495 \t 0.3022586248343154\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 11\n",
      "mean training loss\t 0.34205756075069554\n",
      "epoch 11 '\n",
      "MSE for train and val\t 0.2658454444025427 \t 0.2821253467426775\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 12\n",
      "mean training loss\t 0.3244616859271878\n",
      "epoch 12 '\n",
      "MSE for train and val\t 0.24678265402942517 \t 0.26323308785655725\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 13\n",
      "mean training loss\t 0.30663825747419576\n",
      "epoch 13 '\n",
      "MSE for train and val\t 0.2337093756294768 \t 0.2508554477833055\n",
      "--- 0.986 seconds in epoch ---\n",
      "\n",
      "Epoch 14\n",
      "mean training loss\t 0.2925040854782355\n",
      "epoch 14 '\n",
      "MSE for train and val\t 0.22007735001150028 \t 0.23650895738416441\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 15\n",
      "mean training loss\t 0.2811881588130701\n",
      "epoch 15 '\n",
      "MSE for train and val\t 0.21358913025548207 \t 0.22863896849720436\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 16\n",
      "mean training loss\t 0.2689205253710512\n",
      "epoch 16 '\n",
      "MSE for train and val\t 0.19729028400831677 \t 0.21269534862730366\n",
      "--- 0.975 seconds in epoch ---\n",
      "\n",
      "Epoch 17\n",
      "mean training loss\t 0.25930573481028196\n",
      "epoch 17 '\n",
      "MSE for train and val\t 0.191024255254188 \t 0.2055428164408208\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 18\n",
      "mean training loss\t 0.2516193727000815\n",
      "epoch 18 '\n",
      "MSE for train and val\t 0.18128557135925838 \t 0.19603315669936555\n",
      "--- 0.971 seconds in epoch ---\n",
      "\n",
      "Epoch 19\n",
      "mean training loss\t 0.24406924008345995\n",
      "epoch 19 '\n",
      "MSE for train and val\t 0.17571281020366955 \t 0.18924145759996408\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 20\n",
      "mean training loss\t 0.2377415701991222\n",
      "epoch 20 '\n",
      "MSE for train and val\t 0.16871054470909203 \t 0.18242115630106154\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 21\n",
      "mean training loss\t 0.23333455250888574\n",
      "epoch 21 '\n",
      "MSE for train and val\t 0.16829251320743369 \t 0.1814639736486375\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 22\n",
      "mean training loss\t 0.22827378276918756\n",
      "epoch 22 '\n",
      "MSE for train and val\t 0.16001047312152356 \t 0.17304113564441104\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 23\n",
      "mean training loss\t 0.224534700395631\n",
      "epoch 23 '\n",
      "MSE for train and val\t 0.1560488419006737 \t 0.16843815390209643\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 24\n",
      "mean training loss\t 0.21949443738968646\n",
      "epoch 24 '\n",
      "MSE for train and val\t 0.15418972204160358 \t 0.16589639490750754\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 25\n",
      "mean training loss\t 0.21661034961704348\n",
      "epoch 25 '\n",
      "MSE for train and val\t 0.14969872438668624 \t 0.16183692483719864\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 26\n",
      "mean training loss\t 0.21526056994668774\n",
      "epoch 26 '\n",
      "MSE for train and val\t 0.14817705742743184 \t 0.15962467558226218\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 27\n",
      "mean training loss\t 0.2107952588650047\n",
      "epoch 27 '\n",
      "MSE for train and val\t 0.1447728104804191 \t 0.15607525184496218\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 28\n",
      "mean training loss\t 0.20844372305713715\n",
      "epoch 28 '\n",
      "MSE for train and val\t 0.14406609347412427 \t 0.15503969658628508\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 29\n",
      "mean training loss\t 0.20708160844982648\n",
      "epoch 29 '\n",
      "MSE for train and val\t 0.14205937751509815 \t 0.15281553374660636\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 30\n",
      "mean training loss\t 0.20507570292617455\n",
      "epoch 30 '\n",
      "MSE for train and val\t 0.14577820358353777 \t 0.15612042677479068\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 31\n",
      "mean training loss\t 0.20334647571454284\n",
      "epoch 31 '\n",
      "MSE for train and val\t 0.13857689688439354 \t 0.1495888238180959\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 32\n",
      "mean training loss\t 0.20186868717924494\n",
      "epoch 32 '\n",
      "MSE for train and val\t 0.13702748046671734 \t 0.1477687114175681\n",
      "--- 0.998 seconds in epoch ---\n",
      "\n",
      "Epoch 33\n",
      "mean training loss\t 0.2006567207027654\n",
      "epoch 33 '\n",
      "MSE for train and val\t 0.13578563662710852 \t 0.14626581821362664\n",
      "--- 1.0 seconds in epoch ---\n",
      "\n",
      "Epoch 34\n",
      "mean training loss\t 0.19993199532637831\n",
      "epoch 34 '\n",
      "MSE for train and val\t 0.1348907886487621 \t 0.1453391199177455\n",
      "--- 0.982 seconds in epoch ---\n",
      "\n",
      "Epoch 35\n",
      "mean training loss\t 0.1982840728808622\n",
      "epoch 35 '\n",
      "MSE for train and val\t 0.1342133595575665 \t 0.14456016211769895\n",
      "--- 0.956 seconds in epoch ---\n",
      "\n",
      "Epoch 36\n",
      "mean training loss\t 0.19720822951344194\n",
      "epoch 36 '\n",
      "MSE for train and val\t 0.13303607218142302 \t 0.14332164378671017\n",
      "--- 0.986 seconds in epoch ---\n",
      "\n",
      "Epoch 37\n",
      "mean training loss\t 0.19664190396910808\n",
      "epoch 37 '\n",
      "MSE for train and val\t 0.1330848415405823 \t 0.1435488108841791\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 38\n",
      "mean training loss\t 0.19572329193842217\n",
      "epoch 38 '\n",
      "MSE for train and val\t 0.13260051262564268 \t 0.14295238008113773\n",
      "--- 0.954 seconds in epoch ---\n",
      "\n",
      "Epoch 39\n",
      "mean training loss\t 0.19502110913640164\n",
      "epoch 39 '\n",
      "MSE for train and val\t 0.1312922457321051 \t 0.14126527230043312\n",
      "--- 0.957 seconds in epoch ---\n",
      "\n",
      "Epoch 40\n",
      "mean training loss\t 0.19387256165996927\n",
      "epoch 40 '\n",
      "MSE for train and val\t 0.1323428750645911 \t 0.1426001741634799\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 41\n",
      "mean training loss\t 0.19288153523792986\n",
      "epoch 41 '\n",
      "MSE for train and val\t 0.12961801948769042 \t 0.13959571010108074\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 42\n",
      "mean training loss\t 0.193015348838001\n",
      "epoch 42 '\n",
      "MSE for train and val\t 0.13320754929783604 \t 0.14291001843280118\n",
      "--- 0.979 seconds in epoch ---\n",
      "\n",
      "Epoch 43\n",
      "mean training loss\t 0.19138555267795188\n",
      "epoch 43 '\n",
      "MSE for train and val\t 0.1285116057184386 \t 0.13872007679105156\n",
      "--- 0.985 seconds in epoch ---\n",
      "\n",
      "Epoch 44\n",
      "mean training loss\t 0.19090933692259868\n",
      "epoch 44 '\n",
      "MSE for train and val\t 0.1277962412720056 \t 0.1377158454788457\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 45\n",
      "mean training loss\t 0.19024156954933386\n",
      "epoch 45 '\n",
      "MSE for train and val\t 0.12757057202089747 \t 0.1376698087738808\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 46\n",
      "mean training loss\t 0.18972206430845573\n",
      "epoch 46 '\n",
      "MSE for train and val\t 0.1284403179240718 \t 0.13865912815356338\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 47\n",
      "mean training loss\t 0.18893025407537084\n",
      "epoch 47 '\n",
      "MSE for train and val\t 0.12677536114400834 \t 0.1367412338141362\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 48\n",
      "mean training loss\t 0.1885718847640225\n",
      "epoch 48 '\n",
      "MSE for train and val\t 0.12657807969538395 \t 0.13665120390688862\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 49\n",
      "mean training loss\t 0.18779694855701728\n",
      "epoch 49 '\n",
      "MSE for train and val\t 0.1257426568341517 \t 0.13557985208437037\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 50\n",
      "mean training loss\t 0.18755479161856603\n",
      "epoch 50 '\n",
      "MSE for train and val\t 0.1271480876706732 \t 0.13742108114487533\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 51\n",
      "mean training loss\t 0.1869270328859814\n",
      "epoch 51 '\n",
      "MSE for train and val\t 0.12499109830681167 \t 0.1348493430832793\n",
      "--- 0.955 seconds in epoch ---\n",
      "\n",
      "Epoch 52\n",
      "mean training loss\t 0.18656536748663324\n",
      "epoch 52 '\n",
      "MSE for train and val\t 0.1245260419351565 \t 0.13446638688118648\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.1861136058314902\n",
      "epoch 53 '\n",
      "MSE for train and val\t 0.12410115321465895 \t 0.1340218027492496\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 54\n",
      "mean training loss\t 0.18552955750559197\n",
      "epoch 54 '\n",
      "MSE for train and val\t 0.12380124120391753 \t 0.13355355272583838\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 55\n",
      "mean training loss\t 0.1851053747730177\n",
      "epoch 55 '\n",
      "MSE for train and val\t 0.12396694451408544 \t 0.13365708849924535\n",
      "--- 0.957 seconds in epoch ---\n",
      "\n",
      "Epoch 56\n",
      "mean training loss\t 0.18481028541678288\n",
      "epoch 56 '\n",
      "MSE for train and val\t 0.12559793402238853 \t 0.1355677580974852\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 57\n",
      "mean training loss\t 0.18405240869424383\n",
      "epoch 57 '\n",
      "MSE for train and val\t 0.12297125679593346 \t 0.13272023221745946\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 58\n",
      "mean training loss\t 0.184166545775093\n",
      "epoch 58 '\n",
      "MSE for train and val\t 0.12265002575085573 \t 0.13259021803772322\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 59\n",
      "mean training loss\t 0.18331680178153711\n",
      "epoch 59 '\n",
      "MSE for train and val\t 0.12278895424262336 \t 0.13279422968749607\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 60\n",
      "mean training loss\t 0.18283802942662944\n",
      "epoch 60 '\n",
      "MSE for train and val\t 0.12308016629777169 \t 0.1329405717130403\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 61\n",
      "mean training loss\t 0.18453131398705186\n",
      "epoch 61 '\n",
      "MSE for train and val\t 0.12258736244555081 \t 0.1324990186018374\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 62\n",
      "mean training loss\t 0.18256163487180335\n",
      "epoch 62 '\n",
      "MSE for train and val\t 0.1216680363285754 \t 0.13169058386758623\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 63\n",
      "mean training loss\t 0.18201356618130793\n",
      "epoch 63 '\n",
      "MSE for train and val\t 0.12119507676439649 \t 0.13117627393201067\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 64\n",
      "mean training loss\t 0.1820689001044289\n",
      "epoch 64 '\n",
      "MSE for train and val\t 0.1211403184747886 \t 0.13097861826444543\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 65\n",
      "mean training loss\t 0.18253110378492074\n",
      "epoch 65 '\n",
      "MSE for train and val\t 0.12356184484307114 \t 0.13366951487708612\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 66\n",
      "mean training loss\t 0.1812476260740249\n",
      "epoch 66 '\n",
      "MSE for train and val\t 0.12066677043207448 \t 0.1304902155417167\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 67\n",
      "mean training loss\t 0.18058406307560498\n",
      "epoch 67 '\n",
      "MSE for train and val\t 0.12091735148049935 \t 0.1311538831361355\n",
      "--- 0.957 seconds in epoch ---\n",
      "\n",
      "Epoch 68\n",
      "mean training loss\t 0.180199675750537\n",
      "epoch 68 '\n",
      "MSE for train and val\t 0.12027133594588794 \t 0.1302987041875504\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 69\n",
      "mean training loss\t 0.18008390652840256\n",
      "epoch 69 '\n",
      "MSE for train and val\t 0.12003335386459661 \t 0.1302498321502405\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 70\n",
      "mean training loss\t 0.17973154109032427\n",
      "epoch 70 '\n",
      "MSE for train and val\t 0.12052417507838523 \t 0.13062929337595433\n",
      "--- 0.984 seconds in epoch ---\n",
      "\n",
      "Epoch 71\n",
      "mean training loss\t 0.17940266291137602\n",
      "epoch 71 '\n",
      "MSE for train and val\t 0.11981646537812908 \t 0.13000085377776285\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 72\n",
      "mean training loss\t 0.17917725812704838\n",
      "epoch 72 '\n",
      "MSE for train and val\t 0.11962796807269689 \t 0.12996370610902017\n",
      "--- 0.984 seconds in epoch ---\n",
      "\n",
      "Epoch 73\n",
      "mean training loss\t 0.17900676937376866\n",
      "epoch 73 '\n",
      "MSE for train and val\t 0.11939755848855789 \t 0.12956175340009976\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 74\n",
      "mean training loss\t 0.17855493357924165\n",
      "epoch 74 '\n",
      "MSE for train and val\t 0.11929220959667565 \t 0.12933028325493565\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 75\n",
      "mean training loss\t 0.17949329038135342\n",
      "epoch 75 '\n",
      "MSE for train and val\t 0.11936483342830075 \t 0.12914796123206332\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 76\n",
      "mean training loss\t 0.1784403213223473\n",
      "epoch 76 '\n",
      "MSE for train and val\t 0.1225492202339359 \t 0.13298347040166045\n",
      "--- 0.957 seconds in epoch ---\n",
      "\n",
      "Epoch 77\n",
      "mean training loss\t 0.1780965187510506\n",
      "epoch 77 '\n",
      "MSE for train and val\t 0.11945825116357457 \t 0.12971865276533617\n",
      "--- 0.955 seconds in epoch ---\n",
      "\n",
      "Epoch 78\n",
      "mean training loss\t 0.1775779751724884\n",
      "epoch 78 '\n",
      "MSE for train and val\t 0.1194234880464584 \t 0.1293004637042209\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 79\n",
      "mean training loss\t 0.17735280702348616\n",
      "epoch 79 '\n",
      "MSE for train and val\t 0.11846251685153794 \t 0.12872650485500708\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 80\n",
      "mean training loss\t 0.1777329367448072\n",
      "epoch 80 '\n",
      "MSE for train and val\t 0.11893866164167478 \t 0.12909195132714846\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 81\n",
      "mean training loss\t 0.17735268612865543\n",
      "epoch 81 '\n",
      "MSE for train and val\t 0.1186027543160664 \t 0.12904790530930693\n",
      "--- 0.975 seconds in epoch ---\n",
      "\n",
      "Epoch 82\n",
      "mean training loss\t 0.17678357800010774\n",
      "epoch 82 '\n",
      "MSE for train and val\t 0.11784236329178946 \t 0.12794907243477954\n",
      "--- 0.956 seconds in epoch ---\n",
      "\n",
      "Epoch 83\n",
      "mean training loss\t 0.1764717092279528\n",
      "epoch 83 '\n",
      "MSE for train and val\t 0.11903928588128558 \t 0.12910342667620306\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 84\n",
      "mean training loss\t 0.17645006697685992\n",
      "epoch 84 '\n",
      "MSE for train and val\t 0.11783442889365939 \t 0.12798045401577926\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 85\n",
      "mean training loss\t 0.17653854081376655\n",
      "epoch 85 '\n",
      "MSE for train and val\t 0.11825871615939242 \t 0.12866192280045882\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 86\n",
      "mean training loss\t 0.1759570928626373\n",
      "epoch 86 '\n",
      "MSE for train and val\t 0.11822735057448677 \t 0.12832455719139482\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 87\n",
      "mean training loss\t 0.1759721089337693\n",
      "epoch 87 '\n",
      "MSE for train and val\t 0.11734749788618686 \t 0.12773245219666657\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 88\n",
      "mean training loss\t 0.17586987099198045\n",
      "epoch 88 '\n",
      "MSE for train and val\t 0.11766766613765617 \t 0.12774282785863597\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 89\n",
      "mean training loss\t 0.17582259578783005\n",
      "epoch 89 '\n",
      "MSE for train and val\t 0.11707471689264064 \t 0.1274203099963155\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 90\n",
      "mean training loss\t 0.17540091447165754\n",
      "epoch 90 '\n",
      "MSE for train and val\t 0.11792741100127523 \t 0.12836995802610343\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 91\n",
      "mean training loss\t 0.17516857053412765\n",
      "epoch 91 '\n",
      "MSE for train and val\t 0.1171795509610398 \t 0.1275669555798712\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 92\n",
      "mean training loss\t 0.17546027463967684\n",
      "epoch 92 '\n",
      "MSE for train and val\t 0.11811885361799009 \t 0.12814005044753896\n",
      "--- 0.976 seconds in epoch ---\n",
      "\n",
      "Epoch 93\n",
      "mean training loss\t 0.17535842505634808\n",
      "epoch 93 '\n",
      "MSE for train and val\t 0.11746463293318644 \t 0.12786768222801684\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 94\n",
      "mean training loss\t 0.17462054855022274\n",
      "epoch 94 '\n",
      "MSE for train and val\t 0.11660678421245321 \t 0.12703599003023694\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 95\n",
      "mean training loss\t 0.1748320323033411\n",
      "epoch 95 '\n",
      "MSE for train and val\t 0.11631451542826096 \t 0.12652219618167654\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 96\n",
      "mean training loss\t 0.174389085930879\n",
      "epoch 96 '\n",
      "MSE for train and val\t 0.11638625919208531 \t 0.12674680695505547\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 97\n",
      "mean training loss\t 0.17436280790411057\n",
      "epoch 97 '\n",
      "MSE for train and val\t 0.11636802256017219 \t 0.12664550488836834\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 98\n",
      "mean training loss\t 0.17390515581994762\n",
      "epoch 98 '\n",
      "MSE for train and val\t 0.11911302753192761 \t 0.12904823490143\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 99\n",
      "mean training loss\t 0.17362457169372528\n",
      "epoch 99 '\n",
      "MSE for train and val\t 0.11591729268165758 \t 0.12644650012974065\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 100\n",
      "mean training loss\t 0.1736128614329901\n",
      "epoch 100 '\n",
      "MSE for train and val\t 0.11657078304311223 \t 0.12675000182236795\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 101\n",
      "mean training loss\t 0.17356164086549009\n",
      "epoch 101 '\n",
      "MSE for train and val\t 0.11643594266879674 \t 0.12665830958649738\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 102\n",
      "mean training loss\t 0.17319061365283903\n",
      "epoch 102 '\n",
      "MSE for train and val\t 0.11566187014082105 \t 0.12602873199578743\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 103\n",
      "mean training loss\t 0.17299638857118418\n",
      "epoch 103 '\n",
      "MSE for train and val\t 0.11550321715664534 \t 0.12595694621902764\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 104\n",
      "mean training loss\t 0.17268304629404035\n",
      "epoch 104 '\n",
      "MSE for train and val\t 0.11578361202415459 \t 0.12593098628965452\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 105\n",
      "mean training loss\t 0.17271356760966974\n",
      "epoch 105 '\n",
      "MSE for train and val\t 0.11536510069386309 \t 0.12567259182709875\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.17244941963524116\n",
      "epoch 106 '\n",
      "MSE for train and val\t 0.11523630572809913 \t 0.1256280238496645\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 107\n",
      "mean training loss\t 0.17225870192050935\n",
      "epoch 107 '\n",
      "MSE for train and val\t 0.11511905353846637 \t 0.12563588645849993\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 108\n",
      "mean training loss\t 0.17218747383258382\n",
      "epoch 108 '\n",
      "MSE for train and val\t 0.11506946238063877 \t 0.12548134476061767\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 109\n",
      "mean training loss\t 0.17185345915008765\n",
      "epoch 109 '\n",
      "MSE for train and val\t 0.11500105878737714 \t 0.12537086694205776\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 110\n",
      "mean training loss\t 0.17201564614890053\n",
      "epoch 110 '\n",
      "MSE for train and val\t 0.11502888851856248 \t 0.1254118962377931\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 111\n",
      "mean training loss\t 0.17196653578125062\n",
      "epoch 111 '\n",
      "MSE for train and val\t 0.11506061201367952 \t 0.12559751507423877\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 112\n",
      "mean training loss\t 0.17164295127645868\n",
      "epoch 112 '\n",
      "MSE for train and val\t 0.11500942994192058 \t 0.12558196569147556\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 113\n",
      "mean training loss\t 0.1715664672802706\n",
      "epoch 113 '\n",
      "MSE for train and val\t 0.11499257490940423 \t 0.12557303278927395\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 114\n",
      "mean training loss\t 0.17127833512962842\n",
      "epoch 114 '\n",
      "MSE for train and val\t 0.11460711331836383 \t 0.1251116259771909\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 115\n",
      "mean training loss\t 0.17134377436071146\n",
      "epoch 115 '\n",
      "MSE for train and val\t 0.11483560354777711 \t 0.1253495515399603\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 116\n",
      "mean training loss\t 0.1710286340019742\n",
      "epoch 116 '\n",
      "MSE for train and val\t 0.11444739514033193 \t 0.12472129687658395\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 117\n",
      "mean training loss\t 0.17089349871776144\n",
      "epoch 117 '\n",
      "MSE for train and val\t 0.1142803204697458 \t 0.12474431900701415\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 118\n",
      "mean training loss\t 0.17095516685579643\n",
      "epoch 118 '\n",
      "MSE for train and val\t 0.11430928992859773 \t 0.12461895882732602\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 119\n",
      "mean training loss\t 0.17057233584708856\n",
      "epoch 119 '\n",
      "MSE for train and val\t 0.11543504492661191 \t 0.1261795466083769\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 120\n",
      "mean training loss\t 0.1707950185801162\n",
      "epoch 120 '\n",
      "MSE for train and val\t 0.114244365491179 \t 0.12469008590939176\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 121\n",
      "mean training loss\t 0.17032628889943732\n",
      "epoch 121 '\n",
      "MSE for train and val\t 0.11416266428027637 \t 0.12479265507175803\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 122\n",
      "mean training loss\t 0.1700639258154103\n",
      "epoch 122 '\n",
      "MSE for train and val\t 0.11389368661617243 \t 0.12431591022660483\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 123\n",
      "mean training loss\t 0.170375750128363\n",
      "epoch 123 '\n",
      "MSE for train and val\t 0.11457525898544986 \t 0.12526705775855868\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 124\n",
      "mean training loss\t 0.16992596998566487\n",
      "epoch 124 '\n",
      "MSE for train and val\t 0.11385100961319215 \t 0.12435279962686016\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 125\n",
      "mean training loss\t 0.16994316458213524\n",
      "epoch 125 '\n",
      "MSE for train and val\t 0.1137333374344693 \t 0.12417500210199801\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 126\n",
      "mean training loss\t 0.1698717743646903\n",
      "epoch 126 '\n",
      "MSE for train and val\t 0.11396613693029761 \t 0.12460174359841321\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 127\n",
      "mean training loss\t 0.16984446205076625\n",
      "epoch 127 '\n",
      "MSE for train and val\t 0.11354055917937107 \t 0.12413817460399\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 128\n",
      "mean training loss\t 0.16958027413634003\n",
      "epoch 128 '\n",
      "MSE for train and val\t 0.11381605708235447 \t 0.1244983320614299\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 129\n",
      "mean training loss\t 0.1698123360266451\n",
      "epoch 129 '\n",
      "MSE for train and val\t 0.11394192447066258 \t 0.12456494826271075\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 130\n",
      "mean training loss\t 0.16951259298891316\n",
      "epoch 130 '\n",
      "MSE for train and val\t 0.11338416232431654 \t 0.12391497449403077\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 131\n",
      "mean training loss\t 0.1690980964508213\n",
      "epoch 131 '\n",
      "MSE for train and val\t 0.11336957309137173 \t 0.12397977326468698\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 132\n",
      "mean training loss\t 0.1694086650111636\n",
      "epoch 132 '\n",
      "MSE for train and val\t 0.11335107566019523 \t 0.12378745638476155\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 133\n",
      "mean training loss\t 0.16884099521109316\n",
      "epoch 133 '\n",
      "MSE for train and val\t 0.11342245134636135 \t 0.12385779806406748\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 134\n",
      "mean training loss\t 0.1686700481860364\n",
      "epoch 134 '\n",
      "MSE for train and val\t 0.11317429193452667 \t 0.12362332031769555\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 135\n",
      "mean training loss\t 0.16932079379187256\n",
      "epoch 135 '\n",
      "MSE for train and val\t 0.11332171788617572 \t 0.12371655315523557\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 136\n",
      "mean training loss\t 0.16838768456802994\n",
      "epoch 136 '\n",
      "MSE for train and val\t 0.11306385922350953 \t 0.12376255663947423\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 137\n",
      "mean training loss\t 0.168410416631425\n",
      "epoch 137 '\n",
      "MSE for train and val\t 0.11333725019293162 \t 0.1238116017719749\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 138\n",
      "mean training loss\t 0.16818273546265775\n",
      "epoch 138 '\n",
      "MSE for train and val\t 0.11284131029956948 \t 0.12348598373551452\n",
      "--- 0.978 seconds in epoch ---\n",
      "\n",
      "Epoch 139\n",
      "mean training loss\t 0.16801587184433078\n",
      "epoch 139 '\n",
      "MSE for train and val\t 0.11296312342065666 \t 0.12342403702211444\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 140\n",
      "mean training loss\t 0.16819925848089282\n",
      "epoch 140 '\n",
      "MSE for train and val\t 0.11389910760876373 \t 0.12473512021450857\n",
      "--- 0.956 seconds in epoch ---\n",
      "\n",
      "Epoch 141\n",
      "mean training loss\t 0.16870689709655573\n",
      "epoch 141 '\n",
      "MSE for train and val\t 0.11312141808933582 \t 0.12359644348474091\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 142\n",
      "mean training loss\t 0.1677007037352343\n",
      "epoch 142 '\n",
      "MSE for train and val\t 0.11286761824074804 \t 0.12319482475854188\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 143\n",
      "mean training loss\t 0.16758766623794055\n",
      "epoch 143 '\n",
      "MSE for train and val\t 0.11259117538331884 \t 0.12316825302800617\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 144\n",
      "mean training loss\t 0.16791029281792094\n",
      "epoch 144 '\n",
      "MSE for train and val\t 0.11249585368172595 \t 0.12305630127633663\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 145\n",
      "mean training loss\t 0.16786847251360534\n",
      "epoch 145 '\n",
      "MSE for train and val\t 0.11258570675056973 \t 0.12324412088389775\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 146\n",
      "mean training loss\t 0.16734305864474813\n",
      "epoch 146 '\n",
      "MSE for train and val\t 0.11304790188208705 \t 0.12393158552358326\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 147\n",
      "mean training loss\t 0.1673091883786389\n",
      "epoch 147 '\n",
      "MSE for train and val\t 0.11237234116765857 \t 0.12300133851891434\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 148\n",
      "mean training loss\t 0.1671315301392899\n",
      "epoch 148 '\n",
      "MSE for train and val\t 0.1124897479226639 \t 0.12299066785424359\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 149\n",
      "mean training loss\t 0.16716357865294473\n",
      "epoch 149 '\n",
      "MSE for train and val\t 0.11224136588167452 \t 0.12281602013394013\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 150\n",
      "mean training loss\t 0.16682018725598444\n",
      "epoch 150 '\n",
      "MSE for train and val\t 0.11230805159527543 \t 0.12300217227449693\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 151\n",
      "mean training loss\t 0.1669455404164361\n",
      "epoch 151 '\n",
      "MSE for train and val\t 0.1121439925084032 \t 0.12277163341656698\n",
      "--- 0.983 seconds in epoch ---\n",
      "\n",
      "Epoch 152\n",
      "mean training loss\t 0.16689722745633517\n",
      "epoch 152 '\n",
      "MSE for train and val\t 0.11219433936885818 \t 0.12277758804228439\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 153\n",
      "mean training loss\t 0.16702131919684957\n",
      "epoch 153 '\n",
      "MSE for train and val\t 0.112038768158775 \t 0.12272396273917646\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 154\n",
      "mean training loss\t 0.16703108815384693\n",
      "epoch 154 '\n",
      "MSE for train and val\t 0.11210237683603924 \t 0.12292400855508884\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 155\n",
      "mean training loss\t 0.16645961621257124\n",
      "epoch 155 '\n",
      "MSE for train and val\t 0.11198468371878409 \t 0.12264077600244788\n",
      "--- 0.957 seconds in epoch ---\n",
      "\n",
      "Epoch 156\n",
      "mean training loss\t 0.16642949476105268\n",
      "epoch 156 '\n",
      "MSE for train and val\t 0.11232203297656593 \t 0.12277616331708333\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 157\n",
      "mean training loss\t 0.16630932481073943\n",
      "epoch 157 '\n",
      "MSE for train and val\t 0.11186025355852208 \t 0.12243181965016939\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.1667674321864472\n",
      "epoch 158 '\n",
      "MSE for train and val\t 0.11379602808457885 \t 0.12408118561415381\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 159\n",
      "mean training loss\t 0.16617876613726382\n",
      "epoch 159 '\n",
      "MSE for train and val\t 0.11199979765769877 \t 0.12281776115473127\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 160\n",
      "mean training loss\t 0.1657330925347375\n",
      "epoch 160 '\n",
      "MSE for train and val\t 0.11170807758893574 \t 0.12240788577913739\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 161\n",
      "mean training loss\t 0.16615490190318374\n",
      "epoch 161 '\n",
      "MSE for train and val\t 0.11192822534612845 \t 0.12273408565754987\n",
      "--- 0.994 seconds in epoch ---\n",
      "\n",
      "Epoch 162\n",
      "mean training loss\t 0.16599790207186682\n",
      "epoch 162 '\n",
      "MSE for train and val\t 0.11206700657285466 \t 0.12292008206702715\n",
      "--- 0.987 seconds in epoch ---\n",
      "\n",
      "Epoch 163\n",
      "mean training loss\t 0.16636364689615907\n",
      "epoch 163 '\n",
      "MSE for train and val\t 0.11218163833869559 \t 0.12270592646473606\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 164\n",
      "mean training loss\t 0.1657975800457548\n",
      "epoch 164 '\n",
      "MSE for train and val\t 0.11159398229902849 \t 0.12237087422297277\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 165\n",
      "mean training loss\t 0.16564264072746526\n",
      "epoch 165 '\n",
      "MSE for train and val\t 0.11165366822184983 \t 0.12241563182847844\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 166\n",
      "mean training loss\t 0.1656718870655435\n",
      "epoch 166 '\n",
      "MSE for train and val\t 0.11156294079334864 \t 0.12221261290288649\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 167\n",
      "mean training loss\t 0.16529617421939724\n",
      "epoch 167 '\n",
      "MSE for train and val\t 0.11154921722856789 \t 0.12233164808165\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 168\n",
      "mean training loss\t 0.1652860104060564\n",
      "epoch 168 '\n",
      "MSE for train and val\t 0.11135734277321423 \t 0.12212707057899612\n",
      "--- 0.98 seconds in epoch ---\n",
      "\n",
      "Epoch 169\n",
      "mean training loss\t 0.16527667311859912\n",
      "epoch 169 '\n",
      "MSE for train and val\t 0.11147534530152867 \t 0.12236427692858919\n",
      "--- 0.995 seconds in epoch ---\n",
      "\n",
      "Epoch 170\n",
      "mean training loss\t 0.16578119340978684\n",
      "epoch 170 '\n",
      "MSE for train and val\t 0.11137130183267033 \t 0.12183418735897418\n",
      "--- 0.976 seconds in epoch ---\n",
      "\n",
      "Epoch 171\n",
      "mean training loss\t 0.16510248755822415\n",
      "epoch 171 '\n",
      "MSE for train and val\t 0.11144849568677881 \t 0.12225643357005228\n",
      "--- 1.058 seconds in epoch ---\n",
      "\n",
      "Epoch 172\n",
      "mean training loss\t 0.16489973410231168\n",
      "epoch 172 '\n",
      "MSE for train and val\t 0.11248787697301829 \t 0.12352353069816323\n",
      "--- 1.111 seconds in epoch ---\n",
      "\n",
      "Epoch 173\n",
      "mean training loss\t 0.16511517887721297\n",
      "epoch 173 '\n",
      "MSE for train and val\t 0.11110295577433185 \t 0.12190386106537371\n",
      "--- 1.007 seconds in epoch ---\n",
      "\n",
      "Epoch 174\n",
      "mean training loss\t 0.16512019851657211\n",
      "epoch 174 '\n",
      "MSE for train and val\t 0.11126899570480851 \t 0.12200333567077988\n",
      "--- 1.199 seconds in epoch ---\n",
      "\n",
      "Epoch 175\n",
      "mean training loss\t 0.16520306817332253\n",
      "epoch 175 '\n",
      "MSE for train and val\t 0.11103095588189506 \t 0.12168350190825027\n",
      "--- 1.232 seconds in epoch ---\n",
      "\n",
      "Epoch 176\n",
      "mean training loss\t 0.1645859533157505\n",
      "epoch 176 '\n",
      "MSE for train and val\t 0.11137118776104775 \t 0.122159308083272\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 177\n",
      "mean training loss\t 0.16480056338134358\n",
      "epoch 177 '\n",
      "MSE for train and val\t 0.11095970724851274 \t 0.12161736532497287\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 178\n",
      "mean training loss\t 0.16446162022528102\n",
      "epoch 178 '\n",
      "MSE for train and val\t 0.11092033923062303 \t 0.12162561799104171\n",
      "--- 0.971 seconds in epoch ---\n",
      "\n",
      "Epoch 179\n",
      "mean training loss\t 0.1644361546293634\n",
      "epoch 179 '\n",
      "MSE for train and val\t 0.11163525892238355 \t 0.12255295909326896\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 180\n",
      "mean training loss\t 0.16425559568356296\n",
      "epoch 180 '\n",
      "MSE for train and val\t 0.11080776920489306 \t 0.12153163070271052\n",
      "--- 0.975 seconds in epoch ---\n",
      "\n",
      "Epoch 181\n",
      "mean training loss\t 0.16434794234447792\n",
      "epoch 181 '\n",
      "MSE for train and val\t 0.11097180909404049 \t 0.12163914724109816\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 182\n",
      "mean training loss\t 0.1641943184811561\n",
      "epoch 182 '\n",
      "MSE for train and val\t 0.11082766892450478 \t 0.12163211168077426\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 183\n",
      "mean training loss\t 0.1641742325464233\n",
      "epoch 183 '\n",
      "MSE for train and val\t 0.1107494787345576 \t 0.12140660787639719\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 184\n",
      "mean training loss\t 0.16393509612220233\n",
      "epoch 184 '\n",
      "MSE for train and val\t 0.11075438379756285 \t 0.12142383449675982\n",
      "--- 0.952 seconds in epoch ---\n",
      "\n",
      "Epoch 185\n",
      "mean training loss\t 0.16441483827399425\n",
      "epoch 185 '\n",
      "MSE for train and val\t 0.11131457665890017 \t 0.12225350391065112\n",
      "--- 0.952 seconds in epoch ---\n",
      "\n",
      "Epoch 186\n",
      "mean training loss\t 0.1638411640632348\n",
      "epoch 186 '\n",
      "MSE for train and val\t 0.11102733991614305 \t 0.12192573386689261\n",
      "--- 0.95 seconds in epoch ---\n",
      "\n",
      "Epoch 187\n",
      "mean training loss\t 0.16385821595055158\n",
      "epoch 187 '\n",
      "MSE for train and val\t 0.11066689502137565 \t 0.12137943569236091\n",
      "--- 0.965 seconds in epoch ---\n",
      "\n",
      "Epoch 188\n",
      "mean training loss\t 0.16407919413730745\n",
      "epoch 188 '\n",
      "MSE for train and val\t 0.11058464783030522 \t 0.12126511530484554\n",
      "--- 0.954 seconds in epoch ---\n",
      "\n",
      "Epoch 189\n",
      "mean training loss\t 0.16362337733878465\n",
      "epoch 189 '\n",
      "MSE for train and val\t 0.11048555819503282 \t 0.12127739786436344\n",
      "--- 0.986 seconds in epoch ---\n",
      "\n",
      "Epoch 190\n",
      "mean training loss\t 0.16354567186265695\n",
      "epoch 190 '\n",
      "MSE for train and val\t 0.11060828165447427 \t 0.1213154489617497\n",
      "--- 0.984 seconds in epoch ---\n",
      "\n",
      "Epoch 191\n",
      "mean training loss\t 0.16342010038797972\n",
      "epoch 191 '\n",
      "MSE for train and val\t 0.11060333642617659 \t 0.12120408401823865\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 192\n",
      "mean training loss\t 0.16549251482623523\n",
      "epoch 192 '\n",
      "MSE for train and val\t 0.11361322600947779 \t 0.12392940744637972\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 193\n",
      "mean training loss\t 0.16345899036673248\n",
      "epoch 193 '\n",
      "MSE for train and val\t 0.11062646989843022 \t 0.1215617107332078\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 194\n",
      "mean training loss\t 0.16316011391702245\n",
      "epoch 194 '\n",
      "MSE for train and val\t 0.11046098985678914 \t 0.1211851015674365\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 195\n",
      "mean training loss\t 0.16319867201515886\n",
      "epoch 195 '\n",
      "MSE for train and val\t 0.1102918351846984 \t 0.12111481218524288\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 196\n",
      "mean training loss\t 0.16309929946895504\n",
      "epoch 196 '\n",
      "MSE for train and val\t 0.11045998460331534 \t 0.12118830662947272\n",
      "--- 0.976 seconds in epoch ---\n",
      "\n",
      "Epoch 197\n",
      "mean training loss\t 0.16300696413047977\n",
      "epoch 197 '\n",
      "MSE for train and val\t 0.11023807633331517 \t 0.12098089701999425\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 198\n",
      "mean training loss\t 0.16305514862302875\n",
      "epoch 198 '\n",
      "MSE for train and val\t 0.1104851068679535 \t 0.12145001208436074\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 199\n",
      "mean training loss\t 0.16294412033968284\n",
      "epoch 199 '\n",
      "MSE for train and val\t 0.11046718544009676 \t 0.12105214783384101\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 200\n",
      "mean training loss\t 0.1629475025124237\n",
      "epoch 200 '\n",
      "MSE for train and val\t 0.11023499260251468 \t 0.12098522911484251\n",
      "--- 0.976 seconds in epoch ---\n",
      "\n",
      "Epoch 201\n",
      "mean training loss\t 0.16303999267640662\n",
      "epoch 201 '\n",
      "MSE for train and val\t 0.11035212766860303 \t 0.12123713715419503\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 202\n",
      "mean training loss\t 0.16280741344709865\n",
      "epoch 202 '\n",
      "MSE for train and val\t 0.11062048447603935 \t 0.12161889421525825\n",
      "--- 0.964 seconds in epoch ---\n",
      "\n",
      "Epoch 203\n",
      "mean training loss\t 0.1628178679796516\n",
      "epoch 203 '\n",
      "MSE for train and val\t 0.11007240285456472 \t 0.12078386781380285\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 204\n",
      "mean training loss\t 0.16267446565334914\n",
      "epoch 204 '\n",
      "MSE for train and val\t 0.10996887625891756 \t 0.12071312504088215\n",
      "--- 1.03 seconds in epoch ---\n",
      "\n",
      "Epoch 205\n",
      "mean training loss\t 0.16274139851820274\n",
      "epoch 205 '\n",
      "MSE for train and val\t 0.11065265960560204 \t 0.12156411695600611\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 206\n",
      "mean training loss\t 0.16228462267117422\n",
      "epoch 206 '\n",
      "MSE for train and val\t 0.11049184175267339 \t 0.12096145557880705\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 207\n",
      "mean training loss\t 0.1623765976946862\n",
      "epoch 207 '\n",
      "MSE for train and val\t 0.11030713225140597 \t 0.12089892317848785\n",
      "--- 0.975 seconds in epoch ---\n",
      "\n",
      "Epoch 208\n",
      "mean training loss\t 0.16238848017375979\n",
      "epoch 208 '\n",
      "MSE for train and val\t 0.1105844643959936 \t 0.12159044139653848\n",
      "--- 0.982 seconds in epoch ---\n",
      "\n",
      "Epoch 209\n",
      "mean training loss\t 0.16407220615226714\n",
      "epoch 209 '\n",
      "MSE for train and val\t 0.111147298996525 \t 0.12163088401135651\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.16238690118809215\n",
      "epoch 210 '\n",
      "MSE for train and val\t 0.1098155478379149 \t 0.1205753683234123\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 211\n",
      "mean training loss\t 0.16256543498058787\n",
      "epoch 211 '\n",
      "MSE for train and val\t 0.11108299762409538 \t 0.12220732203294386\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 212\n",
      "mean training loss\t 0.1622765346140158\n",
      "epoch 212 '\n",
      "MSE for train and val\t 0.11043277079700349 \t 0.1215008690481018\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 213\n",
      "mean training loss\t 0.16386498398468144\n",
      "epoch 213 '\n",
      "MSE for train and val\t 0.1110572880457126 \t 0.12154691063436722\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 214\n",
      "mean training loss\t 0.16213243747832345\n",
      "epoch 214 '\n",
      "MSE for train and val\t 0.10995955442463984 \t 0.12065640714228203\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 215\n",
      "mean training loss\t 0.16190862623883076\n",
      "epoch 215 '\n",
      "MSE for train and val\t 0.10965238398196218 \t 0.12043618018989695\n",
      "--- 0.976 seconds in epoch ---\n",
      "\n",
      "Epoch 216\n",
      "mean training loss\t 0.16193037631570315\n",
      "epoch 216 '\n",
      "MSE for train and val\t 0.10967536719422699 \t 0.1204923201734924\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 217\n",
      "mean training loss\t 0.16175227910280227\n",
      "epoch 217 '\n",
      "MSE for train and val\t 0.1095898802905186 \t 0.12041049140509508\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 218\n",
      "mean training loss\t 0.1618045149523704\n",
      "epoch 218 '\n",
      "MSE for train and val\t 0.10965404997880528 \t 0.12045844420437997\n",
      "--- 0.986 seconds in epoch ---\n",
      "\n",
      "Epoch 219\n",
      "mean training loss\t 0.16152185001334207\n",
      "epoch 219 '\n",
      "MSE for train and val\t 0.10953515877829216 \t 0.12026599027911228\n",
      "--- 0.994 seconds in epoch ---\n",
      "\n",
      "Epoch 220\n",
      "mean training loss\t 0.16163319569142137\n",
      "epoch 220 '\n",
      "MSE for train and val\t 0.11029015904080702 \t 0.12082669963024495\n",
      "--- 0.993 seconds in epoch ---\n",
      "\n",
      "Epoch 221\n",
      "mean training loss\t 0.16177616317252644\n",
      "epoch 221 '\n",
      "MSE for train and val\t 0.10951546654336876 \t 0.12024396474275476\n",
      "--- 0.979 seconds in epoch ---\n",
      "\n",
      "Epoch 222\n",
      "mean training loss\t 0.16224751074294574\n",
      "epoch 222 '\n",
      "MSE for train and val\t 0.11042325854503106 \t 0.12094440954407404\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 223\n",
      "mean training loss\t 0.16135231026860533\n",
      "epoch 223 '\n",
      "MSE for train and val\t 0.10944331600687643 \t 0.12017689765619563\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 224\n",
      "mean training loss\t 0.16174716961676958\n",
      "epoch 224 '\n",
      "MSE for train and val\t 0.10939027862015524 \t 0.12021354555721053\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 225\n",
      "mean training loss\t 0.16138485694517854\n",
      "epoch 225 '\n",
      "MSE for train and val\t 0.10972539958245192 \t 0.12045160019754066\n",
      "--- 0.966 seconds in epoch ---\n",
      "\n",
      "Epoch 226\n",
      "mean training loss\t 0.16113989746472876\n",
      "epoch 226 '\n",
      "MSE for train and val\t 0.10962931062036348 \t 0.12058563984774774\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 227\n",
      "mean training loss\t 0.1610462350923507\n",
      "epoch 227 '\n",
      "MSE for train and val\t 0.10929455014294204 \t 0.12011415217391469\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 228\n",
      "mean training loss\t 0.16115336078600806\n",
      "epoch 228 '\n",
      "MSE for train and val\t 0.10948449594181506 \t 0.12018410607059393\n",
      "--- 0.974 seconds in epoch ---\n",
      "\n",
      "Epoch 229\n",
      "mean training loss\t 0.16113551688975974\n",
      "epoch 229 '\n",
      "MSE for train and val\t 0.10971368367005684 \t 0.12029449762131542\n",
      "--- 0.971 seconds in epoch ---\n",
      "\n",
      "Epoch 230\n",
      "mean training loss\t 0.16087046856762932\n",
      "epoch 230 '\n",
      "MSE for train and val\t 0.10927403800300527 \t 0.12006201322636537\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 231\n",
      "mean training loss\t 0.16107590381727843\n",
      "epoch 231 '\n",
      "MSE for train and val\t 0.10938268393974639 \t 0.12005112732130008\n",
      "--- 0.971 seconds in epoch ---\n",
      "\n",
      "Epoch 232\n",
      "mean training loss\t 0.1610055152998596\n",
      "epoch 232 '\n",
      "MSE for train and val\t 0.1092069111142244 \t 0.12006598885297423\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 233\n",
      "mean training loss\t 0.16098776835887157\n",
      "epoch 233 '\n",
      "MSE for train and val\t 0.109387731411344 \t 0.12005446183255206\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 234\n",
      "mean training loss\t 0.16144699349266584\n",
      "epoch 234 '\n",
      "MSE for train and val\t 0.10913902829443814 \t 0.11989540492274295\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 235\n",
      "mean training loss\t 0.16071848422288895\n",
      "epoch 235 '\n",
      "MSE for train and val\t 0.10941167454170007 \t 0.12004482879260017\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 236\n",
      "mean training loss\t 0.160603190787503\n",
      "epoch 236 '\n",
      "MSE for train and val\t 0.10910146268186278 \t 0.11987007212501802\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 237\n",
      "mean training loss\t 0.16047947226000614\n",
      "epoch 237 '\n",
      "MSE for train and val\t 0.10906613115468586 \t 0.11983175756858425\n",
      "--- 0.991 seconds in epoch ---\n",
      "\n",
      "Epoch 238\n",
      "mean training loss\t 0.16044297799712323\n",
      "epoch 238 '\n",
      "MSE for train and val\t 0.10926152551734863 \t 0.11996686891911257\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 239\n",
      "mean training loss\t 0.16035786715687297\n",
      "epoch 239 '\n",
      "MSE for train and val\t 0.1092144009111453 \t 0.12020613603041702\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 240\n",
      "mean training loss\t 0.16037252357260126\n",
      "epoch 240 '\n",
      "MSE for train and val\t 0.10897182093860283 \t 0.11984233272370025\n",
      "--- 0.975 seconds in epoch ---\n",
      "\n",
      "Epoch 241\n",
      "mean training loss\t 0.16045839400565037\n",
      "epoch 241 '\n",
      "MSE for train and val\t 0.10926520931686134 \t 0.12024165508684893\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 242\n",
      "mean training loss\t 0.16152902047653667\n",
      "epoch 242 '\n",
      "MSE for train and val\t 0.10902547979159334 \t 0.11962670193396537\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 243\n",
      "mean training loss\t 0.1603388562065656\n",
      "epoch 243 '\n",
      "MSE for train and val\t 0.10928009485413144 \t 0.12021362648191788\n",
      "--- 0.969 seconds in epoch ---\n",
      "\n",
      "Epoch 244\n",
      "mean training loss\t 0.16026635560833039\n",
      "epoch 244 '\n",
      "MSE for train and val\t 0.1088685960170733 \t 0.11970270001537404\n",
      "--- 0.976 seconds in epoch ---\n",
      "\n",
      "Epoch 245\n",
      "mean training loss\t 0.16027767150128475\n",
      "epoch 245 '\n",
      "MSE for train and val\t 0.10893586489578756 \t 0.11984774926890683\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 246\n",
      "mean training loss\t 0.16015114750041337\n",
      "epoch 246 '\n",
      "MSE for train and val\t 0.10882461331028503 \t 0.11966632229060106\n",
      "--- 0.962 seconds in epoch ---\n",
      "\n",
      "Epoch 247\n",
      "mean training loss\t 0.16003588340321526\n",
      "epoch 247 '\n",
      "MSE for train and val\t 0.10885640772470044 \t 0.11971605609138562\n",
      "--- 0.979 seconds in epoch ---\n",
      "\n",
      "Epoch 248\n",
      "mean training loss\t 0.15990207446891752\n",
      "epoch 248 '\n",
      "MSE for train and val\t 0.1089417572657562 \t 0.11990902080724047\n",
      "--- 1.034 seconds in epoch ---\n",
      "\n",
      "Epoch 249\n",
      "mean training loss\t 0.15988925741832766\n",
      "epoch 249 '\n",
      "MSE for train and val\t 0.10890895827719876 \t 0.11971437535103742\n",
      "--- 0.98 seconds in epoch ---\n",
      "\n",
      "Epoch 250\n",
      "mean training loss\t 0.15993327859972345\n",
      "epoch 250 '\n",
      "MSE for train and val\t 0.10888862449583447 \t 0.11984582484348331\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 251\n",
      "mean training loss\t 0.16124157390145005\n",
      "epoch 251 '\n",
      "MSE for train and val\t 0.10942710940533813 \t 0.12001377516112077\n",
      "--- 0.959 seconds in epoch ---\n",
      "\n",
      "Epoch 252\n",
      "mean training loss\t 0.1600238121679572\n",
      "epoch 252 '\n",
      "MSE for train and val\t 0.10871386157414507 \t 0.11948838531276587\n",
      "--- 0.97 seconds in epoch ---\n",
      "\n",
      "Epoch 253\n",
      "mean training loss\t 0.15968686581635083\n",
      "epoch 253 '\n",
      "MSE for train and val\t 0.10895279053273457 \t 0.11999421238836216\n",
      "--- 1.128 seconds in epoch ---\n",
      "\n",
      "Epoch 254\n",
      "mean training loss\t 0.15962423127694209\n",
      "epoch 254 '\n",
      "MSE for train and val\t 0.10861167503451173 \t 0.11950402914849376\n",
      "--- 1.076 seconds in epoch ---\n",
      "\n",
      "Epoch 255\n",
      "mean training loss\t 0.15953007298903388\n",
      "epoch 255 '\n",
      "MSE for train and val\t 0.1085639108090912 \t 0.11942452889753548\n",
      "--- 1.021 seconds in epoch ---\n",
      "\n",
      "Epoch 256\n",
      "mean training loss\t 0.15947927686034655\n",
      "epoch 256 '\n",
      "MSE for train and val\t 0.10870296120964618 \t 0.11961543914447664\n",
      "--- 1.225 seconds in epoch ---\n",
      "\n",
      "Epoch 257\n",
      "mean training loss\t 0.16006259822943172\n",
      "epoch 257 '\n",
      "MSE for train and val\t 0.10927447035378275 \t 0.11993718811230278\n",
      "--- 1.023 seconds in epoch ---\n",
      "\n",
      "Epoch 258\n",
      "mean training loss\t 0.15984888181823198\n",
      "epoch 258 '\n",
      "MSE for train and val\t 0.10858067610530406 \t 0.11943494538200054\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 259\n",
      "mean training loss\t 0.15947779351082003\n",
      "epoch 259 '\n",
      "MSE for train and val\t 0.10860065539205921 \t 0.11957467925113091\n",
      "--- 1.097 seconds in epoch ---\n",
      "\n",
      "Epoch 260\n",
      "mean training loss\t 0.15927818087769335\n",
      "epoch 260 '\n",
      "MSE for train and val\t 0.10861587855049261 \t 0.11957182086152329\n",
      "--- 1.037 seconds in epoch ---\n",
      "\n",
      "Epoch 261\n",
      "mean training loss\t 0.15943835834499265\n",
      "epoch 261 '\n",
      "MSE for train and val\t 0.10847766899010045 \t 0.11944422054421015\n",
      "--- 0.978 seconds in epoch ---\n",
      "\n",
      "Epoch 262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean training loss\t 0.15917710974568225\n",
      "epoch 262 '\n",
      "MSE for train and val\t 0.10847562339213408 \t 0.11928203227126453\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 263\n",
      "mean training loss\t 0.1592281836466711\n",
      "epoch 263 '\n",
      "MSE for train and val\t 0.10865415033514376 \t 0.11937508764488772\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 264\n",
      "mean training loss\t 0.15913574153282603\n",
      "epoch 264 '\n",
      "MSE for train and val\t 0.10870029887042267 \t 0.11975690803706397\n",
      "--- 0.983 seconds in epoch ---\n",
      "\n",
      "Epoch 265\n",
      "mean training loss\t 0.15943097286537045\n",
      "epoch 265 '\n",
      "MSE for train and val\t 0.10838093264794882 \t 0.11931842468426453\n",
      "--- 0.971 seconds in epoch ---\n",
      "\n",
      "Epoch 266\n",
      "mean training loss\t 0.15902162763427516\n",
      "epoch 266 '\n",
      "MSE for train and val\t 0.10835441272179641 \t 0.11918363199978113\n",
      "--- 0.973 seconds in epoch ---\n",
      "\n",
      "Epoch 267\n",
      "mean training loss\t 0.15893987756772118\n",
      "epoch 267 '\n",
      "MSE for train and val\t 0.10874172525164631 \t 0.11979898753339764\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 268\n",
      "mean training loss\t 0.1591514881272785\n",
      "epoch 268 '\n",
      "MSE for train and val\t 0.10872367278585188 \t 0.11943103930112883\n",
      "--- 0.967 seconds in epoch ---\n",
      "\n",
      "Epoch 269\n",
      "mean training loss\t 0.15932603524845154\n",
      "epoch 269 '\n",
      "MSE for train and val\t 0.1087335520719684 \t 0.1197232897093319\n",
      "--- 0.984 seconds in epoch ---\n",
      "\n",
      "Epoch 270\n",
      "mean training loss\t 0.1591666904629254\n",
      "epoch 270 '\n",
      "MSE for train and val\t 0.10830096079824761 \t 0.1191038369659649\n",
      "--- 0.971 seconds in epoch ---\n",
      "\n",
      "Epoch 271\n",
      "mean training loss\t 0.15929820889332255\n",
      "epoch 271 '\n",
      "MSE for train and val\t 0.11057819212311525 \t 0.12101183507909483\n",
      "--- 0.961 seconds in epoch ---\n",
      "\n",
      "Epoch 272\n",
      "mean training loss\t 0.1587071012278072\n",
      "epoch 272 '\n",
      "MSE for train and val\t 0.10824169266762132 \t 0.11912160226920347\n",
      "--- 0.96 seconds in epoch ---\n",
      "\n",
      "Epoch 273\n",
      "mean training loss\t 0.1593785688036778\n",
      "epoch 273 '\n",
      "MSE for train and val\t 0.10852822766802973 \t 0.11920984565193413\n",
      "--- 0.977 seconds in epoch ---\n",
      "\n",
      "Epoch 274\n",
      "mean training loss\t 0.158702174439782\n",
      "epoch 274 '\n",
      "MSE for train and val\t 0.10819135006253526 \t 0.1190340425251654\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 275\n",
      "mean training loss\t 0.15882440492755076\n",
      "epoch 275 '\n",
      "MSE for train and val\t 0.10930579745111642 \t 0.11985117010084238\n",
      "--- 0.968 seconds in epoch ---\n",
      "\n",
      "Epoch 276\n",
      "mean training loss\t 0.15875308748151434\n",
      "epoch 276 '\n",
      "MSE for train and val\t 0.1081975447311876 \t 0.11912185864803024\n",
      "--- 1.039 seconds in epoch ---\n",
      "\n",
      "Epoch 277\n",
      "mean training loss\t 0.158566260044692\n",
      "epoch 277 '\n",
      "MSE for train and val\t 0.10846039474126518 \t 0.1195332725059907\n",
      "--- 1.036 seconds in epoch ---\n",
      "\n",
      "Epoch 278\n",
      "mean training loss\t 0.15860640142784743\n",
      "epoch 278 '\n",
      "MSE for train and val\t 0.10809821168401505 \t 0.11897833182450743\n",
      "--- 1.045 seconds in epoch ---\n",
      "\n",
      "Epoch 279\n",
      "mean training loss\t 0.15839514605334548\n",
      "epoch 279 '\n",
      "MSE for train and val\t 0.1080732348540195 \t 0.11891555402056958\n",
      "--- 0.975 seconds in epoch ---\n",
      "\n",
      "Epoch 280\n",
      "mean training loss\t 0.1583223160905916\n",
      "epoch 280 '\n",
      "MSE for train and val\t 0.10808862530538908 \t 0.11892745146448223\n",
      "--- 0.972 seconds in epoch ---\n",
      "\n",
      "Epoch 281\n",
      "mean training loss\t 0.1583860073177541\n",
      "epoch 281 '\n",
      "MSE for train and val\t 0.10813779334736981 \t 0.11917246300280511\n",
      "--- 0.979 seconds in epoch ---\n",
      "\n",
      "Epoch 282\n",
      "mean training loss\t 0.15822653819303042\n",
      "epoch 282 '\n",
      "MSE for train and val\t 0.10836000608032964 \t 0.11907935574237066\n",
      "--- 0.982 seconds in epoch ---\n",
      "\n",
      "Epoch 283\n",
      "mean training loss\t 0.1585096039244386\n",
      "epoch 283 '\n",
      "MSE for train and val\t 0.10811647311230449 \t 0.11914498381206771\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 284\n",
      "mean training loss\t 0.15851711601507468\n",
      "epoch 284 '\n",
      "MSE for train and val\t 0.10797301719303067 \t 0.11880377637312184\n",
      "--- 0.963 seconds in epoch ---\n",
      "\n",
      "Epoch 285\n",
      "mean training loss\t 0.15822045011598557\n",
      "epoch 285 '\n",
      "MSE for train and val\t 0.10811465747827467 \t 0.11893960860750015\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 286\n",
      "mean training loss\t 0.15800913648038614\n",
      "epoch 286 '\n",
      "MSE for train and val\t 0.10788930156736003 \t 0.11882104162533987\n",
      "--- 1.016 seconds in epoch ---\n",
      "\n",
      "Epoch 287\n",
      "mean training loss\t 0.15831910408422595\n",
      "epoch 287 '\n",
      "MSE for train and val\t 0.10785904229485123 \t 0.11876648596064639\n",
      "--- 1.018 seconds in epoch ---\n",
      "\n",
      "Epoch 288\n",
      "mean training loss\t 0.15789223882507106\n",
      "epoch 288 '\n",
      "MSE for train and val\t 0.10787880005379454 \t 0.11882814730191504\n",
      "--- 0.981 seconds in epoch ---\n",
      "\n",
      "Epoch 289\n",
      "mean training loss\t 0.157981582083663\n",
      "epoch 289 '\n",
      "MSE for train and val\t 0.10786652409031086 \t 0.11878306818098201\n",
      "--- 0.979 seconds in epoch ---\n",
      "\n",
      "Epoch 290\n",
      "mean training loss\t 0.15781346041648114\n",
      "epoch 290 '\n",
      "MSE for train and val\t 0.10779314816096687 \t 0.11870074257166702\n",
      "--- 0.978 seconds in epoch ---\n",
      "\n",
      "Epoch 291\n",
      "mean training loss\t 0.15805673274348994\n",
      "epoch 291 '\n",
      "MSE for train and val\t 0.10787109618628138 \t 0.11874809840315122\n",
      "--- 1.017 seconds in epoch ---\n",
      "\n",
      "Epoch 292\n",
      "mean training loss\t 0.1578025463907445\n",
      "epoch 292 '\n",
      "MSE for train and val\t 0.10781512941721148 \t 0.11866749797765168\n",
      "--- 0.985 seconds in epoch ---\n",
      "\n",
      "Epoch 293\n",
      "mean training loss\t 0.15781089648848676\n",
      "epoch 293 '\n",
      "MSE for train and val\t 0.10840350023152921 \t 0.11910894728405823\n",
      "--- 0.991 seconds in epoch ---\n",
      "\n",
      "Epoch 294\n",
      "mean training loss\t 0.157677316592365\n",
      "epoch 294 '\n",
      "MSE for train and val\t 0.10779054040071313 \t 0.11872496292717954\n",
      "--- 0.985 seconds in epoch ---\n",
      "\n",
      "Epoch 295\n",
      "mean training loss\t 0.15757547973609362\n",
      "epoch 295 '\n",
      "MSE for train and val\t 0.10792456205245739 \t 0.1187271681086747\n",
      "--- 1.001 seconds in epoch ---\n",
      "\n",
      "Epoch 296\n",
      "mean training loss\t 0.15766904060957862\n",
      "epoch 296 '\n",
      "MSE for train and val\t 0.10775158763171785 \t 0.11862880728821103\n",
      "--- 0.958 seconds in epoch ---\n",
      "\n",
      "Epoch 297\n",
      "mean training loss\t 0.15781527706345574\n",
      "epoch 297 '\n",
      "MSE for train and val\t 0.10769758601248525 \t 0.11870834305301138\n",
      "--- 0.948 seconds in epoch ---\n",
      "\n",
      "Epoch 298\n",
      "mean training loss\t 0.15761386932896787\n",
      "epoch 298 '\n",
      "MSE for train and val\t 0.10766897175509373 \t 0.11862296699220776\n",
      "--- 0.954 seconds in epoch ---\n",
      "\n",
      "Epoch 299\n",
      "mean training loss\t 0.15743080739115106\n",
      "epoch 299 '\n",
      "MSE for train and val\t 0.1076911571901625 \t 0.11866852268108752\n",
      "--- 0.956 seconds in epoch ---\n",
      "\n",
      "FULLY TRAINED USING 292.8332622051239 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from data_loader import loadDataset\n",
    "\n",
    "\n",
    "\n",
    "#batch size, epochs, learning rate\n",
    "BS = 32\n",
    "EP = 300\n",
    "LR = 5e-3\n",
    "\n",
    "data_path = 'data/'\n",
    "trnX1,trnY1,    tstX,tstY = loadDataset(data_path)\n",
    "\n",
    "#whitening the input and output data\n",
    "trnX1 = trnX1 - np.expand_dims(np.mean(trnX1,axis=0),axis=0)\n",
    "trnX1 = trnX1 / np.expand_dims(np.sqrt(np.mean(trnX1**2,axis=0)),axis=0)\n",
    "trnX1_og = trnX1    \n",
    "trnY1_og = trnY1[:,None]-4.42\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "#for mn in range(5):\n",
    "if True:\n",
    "    mn=0\n",
    "    M_NUM = trnX1_og.shape[0]\n",
    "    rand_indices = np.random.permutation(M_NUM)\n",
    "    per = .7\n",
    "    M_TRN_NUM = int(M_NUM*per)\n",
    "    trnX1 = trnX1_og[rand_indices[:M_TRN_NUM]]\n",
    "    trnY1 = trnY1_og[rand_indices[:M_TRN_NUM]]\n",
    "    valX1 = trnX1_og[rand_indices[M_TRN_NUM:]]\n",
    "    valY1 = trnY1_og[rand_indices[M_TRN_NUM:]]\n",
    "\n",
    "\n",
    "    trn_data1 = TensorDataset(torch.from_numpy(trnX1).float().to(device),\n",
    "                              torch.from_numpy(trnY1).float().to(device))\n",
    "    trn_loader1 =DataLoader(dataset=trn_data1, batch_size=BS,shuffle=True)\n",
    "\n",
    "    trn_tensor = torch.from_numpy(trnX1).float().to(device)\n",
    "    val_tensor = torch.from_numpy(valX1).float().to(device)\n",
    "\n",
    "\n",
    "    sizes = [13,256,128,64,1]\n",
    "    sian_net1 = SIAN(sizes,indices,False).to(device)\n",
    "    \n",
    "    opt1 = optim.Adagrad(sian_net1.parameters(), lr = LR)\n",
    "    \n",
    "    \n",
    "    all_trn_accs = np.zeros(EP)\n",
    "    all_val_accs = np.zeros(EP)\n",
    "    all_losses = np.zeros((EP,len(trn_loader1),7))\n",
    "    all_trn_losses = np.zeros((EP,len(trn_loader1)))\n",
    "    all_val_losses = np.zeros(EP)\n",
    "\n",
    "    best_val_score = -100\n",
    "    best_net2 = None\n",
    "\n",
    "    full_training_start_time = time.time()\n",
    "    for k in range(EP):\n",
    "        print('Epoch',k)\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "        #---TRAINING PHASE---\n",
    "        for j,(x_batch,y_batch) in enumerate(trn_loader1):\n",
    "            sian_net1.train()\n",
    "            logits = sian_net1(x_batch)\n",
    "            dnn_logits,gam_logits,shape_loss = sian_net1(x_batch)\n",
    "            logits = dnn_logits + gam_logits\n",
    "            \n",
    "\n",
    "            l1_reg = torch.zeros(1).to(device)\n",
    "            all_linear_params = sian_net1.collectParameters()\n",
    "            lambda1 = 5e-5 \n",
    "            l1_reg = lambda1 * torch.norm(all_linear_params, 1)\n",
    "\n",
    "\n",
    "            mseloss = (y_batch.narrow(1,0,1) - logits.narrow(1,0,1))**2\n",
    "            mseloss = torch.mean(mseloss)\n",
    "            mseloss = mseloss\n",
    "            \n",
    "            \n",
    "            all_trn_losses[k,j] = mseloss.item()\n",
    "            loss = mseloss + l1_reg\n",
    "            loss.backward()\n",
    "            all_losses[k,j,0] = loss.item()\n",
    "            all_losses[k,j,1] = mseloss.item()\n",
    "            all_losses[k,j,2] = l1_reg.item()\n",
    "            \n",
    "            \n",
    "            opt1.step()\n",
    "            opt1.zero_grad()\n",
    "        print('mean training loss\\t',np.mean(all_losses[k,:,0]))\n",
    "        \n",
    "        \n",
    "        #---VALIDATION PHASE---\n",
    "        print('epoch',k,'\\'')\n",
    "        sian_net1.eval()\n",
    "        #training accuracy\n",
    "        logits = sian_net1(trn_tensor)\n",
    "        logits = logits[0]+logits[1]\n",
    "        logits = logits.cpu().detach().numpy()[:,0]\n",
    "        all_trn_accs[k] = np.mean(((trnY1[:,0])-logits)**2)\n",
    "        #validation accuracy\n",
    "        logits = sian_net1(val_tensor)\n",
    "        logits = logits[0]+logits[1]\n",
    "        logits = logits.cpu().detach().numpy()[:,0]\n",
    "        all_val_accs[k] = np.mean(((valY1[:,0])-logits)**2)\n",
    "        all_val_losses[k] = np.mean(((valY1[:,0])-logits)**2)\n",
    "        print('MSE for train and val\\t',all_trn_accs[k],'\\t',all_val_accs[k])\n",
    "\n",
    "\n",
    "        \n",
    "        if k%1==0:\n",
    "            val_score = -all_val_accs[k]\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                best_net2 = copy.deepcopy(sian_net1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"--- %s seconds in epoch ---\" % round((end_time - start_time),3))\n",
    "        print()\n",
    "    print('FULLY TRAINED USING',time.time()-full_training_start_time,'seconds')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.mean(all_losses[:k,:,0],axis=1))\n",
    "plt.plot(all_val_losses[:k])\n",
    "plt.plot(np.mean(all_losses[:k,:,1],axis=1))\n",
    "plt.plot(np.mean(all_losses[:k,:,2],axis=1))\n",
    "plt.plot([-50,EP+50],[0,0],c='k')\n",
    "plt.legend(['trn','val','mse','l1'])\n",
    "plt.xlim(-20,EP+20)\n",
    "plt.ylim(-.01,.45)\n",
    "plt.title(\"SIAN Loss While Training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
